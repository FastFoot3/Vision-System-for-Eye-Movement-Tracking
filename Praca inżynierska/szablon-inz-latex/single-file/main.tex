% !TeX spellcheck = pl_PL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
% Szablon pracy dyplomowej inzynierskiej %
% zgodny  z aktualnymi  przepisami  SZJK %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%  (c) Krzysztof Simiński, 2018-2023     %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
% Najnowsza wersja szablonów jest        %
% podstępna pod adresem                  %
% github.com/ksiminski/polsl-aei-theses  %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% Projekt LaTeXowy zapewnia odpowiednie formatowanie pracy,
% zgodnie z wymaganiami Systemu zapewniania jakości kształcenia.
% Proszę nie zmieniać ustawień formatowania (np. fontu,
% marginesów, wytłuszczeń, kursywy itd. ).
%
% Projekt można kompilować na kilka sposobów.
%
% 1. kompilacja pdfLaTeX
%
% pdflatex main
% bibtex   main
% pdflatex main
% pdflatex main
%
%
% 2. kompilacja XeLaTeX
%
% Kompilatacja przy użyciu XeLaTeXa różni się tym, że na stronie
% tytułowej używany jest font Calibri. Wymaga to jego uprzedniego
% zainstalowania.
%
% xelatex main
% bibtex  main
% xelatex main
% xelatex main
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% W przypadku pytań, uwag, proszę pisać na adres:   %
%      krzysztof.siminski(małpa)polsl.pl            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chcemy ulepszać szablony LaTeXowe prac dyplomowych.
% Wypełniając ankietę spod poniższego adresu pomogą
% Państwo nam to zrobić. Ankieta jest całkowicie
% anonimowa. Dziękujemy!


% https://docs.google.com/forms/d/e/1FAIpQLScyllVxNKzKFHfILDfdbwC-jvT8YL0RSTFs-s27UGw9CKn-fQ/viewform?usp=sf_link
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% PERSONALIZACJA PRACY – DANE PRACY           %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Proszę wpisać swoje dane w poniższych definicjach.

%--------------------------------------
%
%
% TODO: None
%
% Status: 100% (DONE)
%--------------------------------------
% dane autora
\newcommand{\FirstNameAuthor}{Bartosz}
\newcommand{\SurnameAuthor}{Wuwer}
\newcommand{\IdAuthor}{296949}   % numer albumu  (bez $\langle$ i $\rangle$)

% drugi autor:
%\newcommand{\FirstNameCoauthor}{Imię}   % Jeżeli jest drugi autor, to tutaj należy podać imię.
%\newcommand{\SurnameCoauthor}{Nazwisko} % Jeżeli jest drugi autor, to tutaj należy podać nazwisko.
%\newcommand{\IdCoauthor}{$\langle$wpisać właściwy$\rangle$}  % numer albumu drugiego autora (bez $\langle$ i $\rangle$)
% Gdy nie ma drugiego autora, należy zostawić poniższe definicje puste, jak poniżej. Gdy jest drugi autor, należy zakomentować te linie.
\newcommand{\FirstNameCoauthor}{} % Jeżeli praca ma tylko jednego autora, to dane drugiego autora zostają puste.
\newcommand{\SurnameCoauthor}{}   % Jeżeli praca ma tylko jednego autora, to dane drugiego autora zostają puste.
\newcommand{\IdCoauthor}{}  % Jeżeli praca ma tylko jednego autora, to dane drugiego autora zostają puste.
%%%%%%%%%%

\newcommand{\Supervisor}{Dr inż. Krzysztof Jaskot}     % dane promotora (bez $\langle$ i $\rangle$)
\newcommand{\Title}{System wizyjny do śledzenia ruchu gałek ocznych}           % tytuł pracy po polsku
\newcommand{\TitleAlt}{Vision system for tracking the movement of the eyeballs}                     % thesis title in English
\newcommand{\Program}{Automatyka i Robotyka}            % kierunek studiów  (bez $\langle$ i $\rangle$)
\newcommand{\Specialisation}{Technologie informacyjne w automatyce i robotyce}     % specjalność  (bez $\langle$ i $\rangle$)
\newcommand{\Departament}{Automatyki i Robotyki}        % katedra promotora  (bez $\langle$ i $\rangle$)

% Jeżeli został wyznaczony promotor pomocniczy lub opiekun, proszę go/ją wpisać ...
\newcommand{\Consultant}{} % dane promotora pomocniczego, opiekuna (bez $\langle$ i $\rangle$)
% ... w przeciwnym razie proszę zostawić puste miejsce jak poniżej:
%\newcommand{\Consultant}{} % brak promotowa pomocniczego / opiekuna

% koniec fragmentu do modyfikacji
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% KONIEC PERSONALIZACJI PRACY                 %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% PROSZĘ NIE MODYFIKOWAĆ PONIŻSZYCH USTAWIEŃ! %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[a4paper,twoside,12pt]{book}
\usepackage[utf8]{inputenc}                                      
\usepackage[T1]{fontenc}  
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[british,polish]{babel} 
\usepackage{indentfirst}
\usepackage{xurl}
\usepackage{xstring}
\usepackage{ifthen}



\usepackage{ifxetex}

\ifxetex
	\usepackage{fontspec}
	\defaultfontfeatures{Mapping=tex—text} % to support TeX conventions like ``——-''
	\usepackage{xunicode} % Unicode support for LaTeX character names (accents, European chars, etc)
	\usepackage{xltxtra} % Extra customizations for XeLaTeX
\else
	\usepackage{lmodern}
\fi



\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{subcaption}   % subfigures
\usepackage[page]{appendix} % toc,
\renewcommand{\appendixtocname}{Dodatki}
\renewcommand{\appendixpagename}{Dodatki}
\renewcommand{\appendixname}{Dodatek}

\usepackage{csquotes}
\usepackage[natbib=true,backend=bibtex,maxbibnames=99]{biblatex}  % kompilacja bibliografii BibTeXem
%\usepackage[natbib=true,backend=biber,maxbibnames=99]{biblatex}  % kompilacja bibliografii Biberem
\bibliography{biblio}

\usepackage{ifmtarg}   % empty commands  

\usepackage{setspace}
\onehalfspacing


\frenchspacing



%%%% TODO LIST GENERATOR %%%%%%%%%

\usepackage{color}
\definecolor{brickred}      {cmyk}{0   , 0.89, 0.94, 0.28}

\makeatletter \newcommand \kslistofremarks{\section*{Uwagi} \@starttoc{rks}}
  \newcommand\l@uwagas[2]
    {\par\noindent \textbf{#2:} %\parbox{10cm}
{#1}\par} \makeatother


\newcommand{\ksremark}[1]{%
{%\marginpar{\textdbend}
{\color{brickred}{[#1]}}}%
\addcontentsline{rks}{uwagas}{\protect{#1}}%
}

\newcommand{\comma}{\ksremark{przecinek}}
\newcommand{\nocomma}{\ksremark{bez przecinka}}
\newcommand{\styl}{\ksremark{styl}}
\newcommand{\ortografia}{\ksremark{ortografia}}
\newcommand{\fleksja}{\ksremark{fleksja}}
\newcommand{\pauza}{\ksremark{pauza `--', nie dywiz `-'}}
\newcommand{\kolokwializm}{\ksremark{kolokwializm}}
\newcommand{\cudzyslowy}{\ksremark{,,polskie cudzysłowy''}}

%%%%%%%%%%%%%% END OF TODO LIST GENERATOR %%%%%%%%%%%

\newcommand{\printCoauthor}{%		
    \StrLen{\FirstNameCoauthor}[\FNCoALen]
    \ifthenelse{\FNCoALen > 0}%
    {%
		{\large\bfseries\Coauthor\par}
	
		{\normalsize\bfseries \LeftId: \IdCoauthor\par}
    }%
    {}
} 

%%%%%%%%%%%%%%%%%%%%%
\newcommand{\autor}{%		
    \StrLen{\FirstNameCoauthor}[\FNCoALenXX]
    \ifthenelse{\FNCoALenXX > 0}%
    {\FirstNameAuthor\ \SurnameAuthor, \FirstNameCoauthor\ \SurnameCoauthor}%
	{\FirstNameAuthor\ \SurnameAuthor}%
}
%%%%%%%%%%%%%%%%%%%%%

\StrLen{\FirstNameCoauthor}[\FNCoALen]
\ifthenelse{\FNCoALen > 0}%
{%
\author{\FirstNameAuthor\ \SurnameAuthor, \FirstNameCoauthor\ \SurnameCoauthor}
}%
{%
\author{\FirstNameAuthor\ \SurnameAuthor}
}%

%%%%%%%%%%%% ZYWA PAGINA %%%%%%%%%%%%%%%
% brak kapitalizacji zywej paginy
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\it\rightmark}}
\fancyhead[RE]{\nouppercase{\it\leftmark}}
\fancyhead[LE,RO]{\it\thepage}


\fancypagestyle{tylkoNumeryStron}{%
   \fancyhf{} 
   \fancyhead[LE,RO]{\it\thepage}
}

\fancypagestyle{bezNumeracji}{%
   \fancyhf{} 
   \fancyhead[LE,RO]{}
}


\fancypagestyle{NumeryStronNazwyRozdzialow}{%
   \fancyhf{} 
   \fancyhead[LE]{\nouppercase{\autor}}
   \fancyhead[RO]{\nouppercase{\leftmark}} 
   \fancyfoot[CE, CO]{\thepage}
}


%%%%%%%%%%%%% OBCE WTRETY  
\newcommand{\obcy}[1]{\emph{#1}}
\newcommand{\english}[1]{{\selectlanguage{british}\obcy{#1}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% polskie oznaczenia funkcji matematycznych
\renewcommand{\tan}{\operatorname {tg}}
\renewcommand{\log}{\operatorname {lg}}

% jeszcze jakies drobiazgi

\newcounter{stronyPozaNumeracja}

%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newcommand{\printOpiekun}[1]{%		

    \StrLen{\Consultant}[\mystringlen]
    \ifthenelse{\mystringlen > 0}%
    {%
       {\large{\bfseries OPIEKUN, PROMOTOR POMOCNICZY}\par}
       
       {\large{\bfseries \Consultant}\par}
    }%
    {}
} 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
% Proszę nie modyfikować poniższych definicji!
\newcommand{\Author}{\FirstNameAuthor\ \MakeUppercase{\SurnameAuthor}} 
\newcommand{\Coauthor}{\FirstNameCoauthor\ \MakeUppercase{\SurnameCoauthor}}
\newcommand{\Type}{PROJEKT INŻYNIERSKI}
\newcommand{\Faculty}{Wydział Automatyki, Elektroniki i Informatyki} 
\newcommand{\Polsl}{Politechnika Śląska}
\newcommand{\Logo}{politechnika_sl_logo_bw_pion_pl.pdf}
\newcommand{\LeftId}{Nr albumu}
\newcommand{\LeftProgram}{Kierunek}
\newcommand{\LeftSpecialisation}{Specjalność}
\newcommand{\LeftSUPERVISOR}{PROWADZĄCY PRACĘ}
\newcommand{\LeftDEPARTMENT}{KATEDRA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% KONIEC USTAWIEŃ                             %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% MOJE PAKIETY, USTAWIENIA ITD                %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Tutaj proszę umieszczać swoje pakiety, makra, ustawienia itd.

\usepackage{siunitx}
\sisetup{locale = PL}

\usepackage{float}

\usepackage{placeins}
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% listingi i fragmentu kodu źródłowego 
% pakiet: listings lub minted
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

% biblioteka listings
\usepackage{listings}
\lstset{%
morekeywords={string,exception,std,vector},% słowa kluczowe rozpoznawane przez pakiet listings
language=Python,% C, Matlab, Python, SQL, TeX, XML, bash, ... – vide https://www.ctan.org/pkg/listings
commentstyle=\textit,%
identifierstyle=\textsf,%
keywordstyle=\sffamily\bfseries, %\texttt, %
%captionpos=b,%
tabsize=3,%
frame=lines,%
numbers=left,%
numberstyle=\tiny,%
numbersep=5pt,%
breaklines=true,%
escapeinside={@*}{*@},%
literate={ą}{{\k{a}}}1 {Ą}{{\k{A}}}1 {ę}{{\k{e}}}1 {Ę}{{\k{E}}}1 {ó}{{\'o}}1 {Ó}{{\'O}}1 {ś}{{\'s}}1 {Ś}{{\'S}}1 {ł}{{\l{}}}1 {Ł}{{\L{}}}1 {ż}{{\.z}}1 {Ż}{{\.Z}}1 {ź}{{\'z}}1 {Ź}{{\'Z}}1 {ć}{{\'c}}1 {Ć}{{\'C}}1 {ń}{{\'n}}1 {Ń}{{\'N}}1,%
}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% pakiet minted
%\usepackage{minted}

% pakiet wymaga specjalnego kompilowania:
% pdflatex -shell-escape main.tex
% xelatex  -shell-escape main.tex

%\usepackage[chapter]{minted} % [section]
%%\usemintedstyle{bw}   % czarno-białe kody 
%
%\setminted % https://ctan.org/pkg/minted
%{
%%fontsize=\normalsize,%\footnotesize,
%%captionpos=b,%
%tabsize=3,%
%frame=lines,%
%framesep=2mm,
%numbers=left,%
%numbersep=5pt,%
%breaklines=true,%
%escapeinside=@@,%
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% KONIEC MOICH USTAWIEŃ                       %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
%\kslistofremarks

\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% PROSZĘ NIE MODYFIKOWAĆ STRONY TYTUŁOWEJ!    %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  STRONA TYTUŁOWA %%%%%%%%%%%%%%%%%%%
\pagestyle{empty}
{
	\newgeometry{top=1.5cm,%
	             bottom=2.5cm,%
	             left=3cm,
	             right=2.5cm}
 
	\ifxetex 
	  \begingroup
	  \setsansfont{Calibri}
	   
	\fi 
	 \sffamily
	\begin{center}
	\includegraphics[width=50mm]{\Logo}
	 
	
	{\Large\bfseries\Type\par}
	
	\vfill  \vfill  
			 
	{\large\Title\par}
	
	\vfill  
		
	{\large\bfseries\Author\par}
	
	{\normalsize\bfseries \LeftId: \IdAuthor}

	\printCoauthor
	
	\vfill  		
 
	{\large{\bfseries \LeftProgram:} \Program\par} 
	
	{\large{\bfseries \LeftSpecialisation:} \Specialisation\par} 
	 		
	\vfill  \vfill 	\vfill 	\vfill 	\vfill 	\vfill 	\vfill  
	 
	{\large{\bfseries \LeftSUPERVISOR}\par}
	
	{\large{\bfseries \Supervisor}\par}
				
	{\large{\bfseries \LeftDEPARTMENT\ \Departament} \par}
		
	{\large{\bfseries \Faculty}\par}
		
	\vfill  \vfill  

    	
    \printOpiekun{\Consultant}
    
	\vfill  \vfill  
		
    {\large\bfseries  Gliwice \the\year}

   \end{center}	
       \ifxetex 
       	  \endgroup
       \fi
	\restoregeometry
}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% KONIEC STRONY TYTUŁOWEJ                     %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  


\cleardoublepage

\rmfamily\normalfont
\pagestyle{empty}


%%% No to zaczynamy pisać pracę :-) %%%%

%--------------------------------------
%
%
% TODO: None
%
% Status: 100% (DONE)
%--------------------------------------
\subsubsection*{Tytuł pracy} 
\Title

\subsubsection*{Streszczenie}  

Praca inżynierska dotyczy opracowania systemu do wykrywania i śledzenia ruchu oczu w czasie rzeczywistym, wykorzystującego metody przetwarzania obrazu i uczenia maszynowego.

Projekt koncentruje się na stworzeniu rozwiązania umożliwiającego detekcję oczu oraz dalszą analizę ruchu źrenic, co może znaleźć zastosowanie w testowaniu interfejsów użytkownika, badaniach nad percepcją wzrokową oraz innych dziedzinach wymagających śledzenia wzroku.

Do implementacji systemu wykorzystano język Python oraz biblioteki o otwartym kodzie źródłowym, głównie OpenCV. W systemie zastosowano klasyfikatory Haara do wykrycia twarzy i oczu oraz operacje binaryzacji i morfologii matematycznej w celu precyzyjnej lokalizacji źrenic. Opracowano również trzy metody kalibracji, pozwalające dostosować działanie systemu do różnych warunków oświetleniowych i preferencji użytkownika.

System został przetestowany w różnych warunkach, a wyniki analizy pozwoliły na poprawę stabilności wykrywania oczu. Opracowane rozwiązanie stanowi punkt wyjścia do dalszych badań nad algorytmami eye trackingu i może być rozwijane w przyszłości.

\subsubsection*{Słowa kluczowe} 
Okulografia, Przetwarzanie obrazu, Detekcja w czasie rzeczywistym, Uczenie maszynowe, Wizja komputerowa

\subsubsection*{Thesis title} 
\begin{otherlanguage}{british}
\TitleAlt
\end{otherlanguage}

\subsubsection*{Abstract} 
\begin{otherlanguage}{british}
	This engineering thesis focuses on the development of a real-time eye detection and tracking system using image processing and machine learning methods.

	The project aims to create a solution that enables eye detection and further pupil movement analysis, which can be applied in user interface testing, research on visual perception, and other fields requiring eye tracking technology.
	
	The system was implemented in Python using open-source libraries, primarily OpenCV. It employs Haar classifiers for face and eye detection, along with binarization and morphological operations to precisely locate pupils. Additionally, three calibration methods were developed to adjust the system's functionality to various lighting conditions and user preferences.
	
	The system was tested under different conditions, and the analysis results contributed to improving the stability of eye detection. The developed solution serves as a foundation for further research on eye tracking algorithms and can be expanded in the future.
\end{otherlanguage}
\subsubsection*{Key words}  
\begin{otherlanguage}{british}
	Eye tracking, Image processing, Real-time detection, Machine learning, Computer vision
\end{otherlanguage}




%%%%%%%%%%%%%%%%%% SPIS TRESCI %%%%%%%%%%%%%%%%%%%%%%
% Add \thispagestyle{empty} to the toc file (main.toc), because \pagestyle{empty} doesn't work if the TOC has multiple pages
\addtocontents{toc}{\protect\thispagestyle{empty}}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{stronyPozaNumeracja}{\value{page}}
\mainmatter
\pagestyle{empty}

\cleardoublepage

\pagestyle{NumeryStronNazwyRozdzialow}

%%%%%%%%%%%%%% wlasciwa tresc pracy %%%%%%%%%%%%%%%%%

%--------------------------------------
%
%
% TODO: 
%
% Status: 100%
%--------------------------------------
\chapter{Wstęp}
\label{ch:wstep}

\section{Wprowadzenie i osadzenie w dziedzinie}
\label{sec:wprowadzenie-i-osadzenie-w-dziedzinie}
% wprowadzenie w problem/zagadnienie
Śledzenie ruchu gałek ocznych (ang. \english{eye tracking}), zwane również jako okulografia, jest techniką, która badana jest od ponad stu lat. Istotność tej techniki wynika z faktu, że ruchy gałek ocznych są ściśle związane z procesami poznawczymi, takimi jak uwaga, percepcja, pamięć, czy procesy decyzyjne. Skupiając wzrok na danym punkcie, umieszczamy go w centralnym obszarze naszego pola widzenia które charakteryzuje się największą rozdzielczością, co pozwala na dokładne analizowanie szczegółów. Ten fakt wpływa także na proces skupienia -- gdy koncentrujemy się na danym obiekcie lub obszarze, skupiamy na nim wzrok (często wystarczy jedynie krótki moment).

Możliwość rejestrowania ruchów oczu pozwala na zrozumienie w jaki sposób obserwator eksploruje otaczający go świat. Posiadając tą wiedzę możliwe jest wyciągnięcie wniosków na temat tego co jest interesujące lub istotne dla obserwatora, jakie emocje się z tym wiążą, czy nawet jakie procesy poznawcze zachodzą w jego umyśle, czy rozumie on to co widzi. Nie trudnym jest zauważyć jak cenne mogą być te informacje w szerokim spektrum dziedzin.

% osadzenie problemu w dziedzinie
Okulografia odgrywa kluczową rolę w psychologi poznawczej, psychologi społecznej, neurobiologii, marketingu, czy medycynie. W psychologii poznawczej ruch oczu jest ściśle związany z pamięcią, podejmowaniem decyzji, obciążeniem poznawczym i uczeniem się asocjacyjnym. W psychologii społecznej eye tracking pozwala na wgląd w zachowania społeczne i ich analizę, co pozwala badać empatię, prospołeczność, czy fobie społeczne \cite{bib:tobii-main}. W neurobiologii bada się powiązania ruchu oczu ze szlakami neuronowymi odpowiedzialnymi za podejmowane akcje i procesy myślowe, dając możliwość w diagnozach i wsparciu osób dotkniętych chorobą Parkinsona \cite{bib:tobii-parkinson}, Alzheimera \cite{bib:tobii-alzheimer}, a także autyzmem czy łagodnym upośledzeniem funkcji poznawczych \cite{bib:tobii-autyzm}. W medycynie okulografia pozwala na diagnozę między innymi oczopląsu, który może być objawem np. stwardnienia rozsianego, uszkodzenia śródmózgowia lub urazu ucha wewnętrznego \cite{bib:Diagnostyka-oczoplas}. Niezaprzeczalnie technika ta jest niezwykle cenna i efektywna na wielu płaszczyznach naukowych i praktycznych.

\section{Cel oraz zakres pracy}
\label{sec:cel-oraz-zakres-pracy}
% cel pracy
Celem niniejszej pracy jest wykonanie narzędzia które pozwoli na uniwersalne śledzenie ruchu gałek ocznych, bez zdefiniowanej docelowej grupy użytkowników. System ten powinien być prosty w obsłudze i niewymagający zaawansowanej technologii, ale nie ograniczający w razie potrzeby bardziej zaawansowanych użytkowników, prezentując użyteczność zarówno dla naukowców, studentów, jak i hobbystów. Tego typu narzędzie pozwoli na eksplorację danych eye-trackingowych, otwierając możliwości na dalszą analizę i interpretację wyników, a także zapewni solidną podstawę w razie potrzeby modyfikacji lub rozbudowy systemu w bardziej ukierunkowany sposób.

% zakres pracy
Praca ta skupia się przede wszystkim na samym procesie wykrywania źrenic i śledzenia ruchu gałek ocznych na ich podstawie oraz implementacji systemu wizyjnego, który pozwoli na zapis, wizualizację i potencjalną dalszą analizę danych zebranych przez kamerę, w tym także kamerę internetową. Całość zrealizowana jest w języku Python, z wykorzystaniem bibliotek takich jak OpenCV, NumPy, pandas oraz Matplotlib, a wszelkie prezentowane dane zostały uchwycone przy użyciu wbudowanej kamery laptopa. Praca nie będzie obejmować rozległej analizy zebranych danych, ani eksperymentów przeprowadzonych z użyciem owego systemu. Nie została także stworzona grupa testowa, więc większość testów przeprowadzone zostały na autorze pracy.

\section{Charakterystyka rozdziałów}

\begin{description}
	\item[Wstęp] Przedstawia cel i zakres pracy, uzasadnia wybór tematu oraz opisuje strukturę dokumentu.
	\item[Śledzenie ruchu gałek ocznych -- analiza tematu] Omawia podstawowe zagadnienia związane z eye trackingiem, w tym mechanizmy ruchu oczu, istniejące metody detekcji oraz ich zastosowania.
	\item[Wymagania i narzędzia] Definiuje wymagania dla systemu oraz opisuje wykorzystane narzędzia, zarówno sprzętowe, jak i programistyczne.
	\item[Specyfikacja zewnętrzna] Opisuje sposób użytkowania systemu, jego interfejs oraz grupę docelową, do której jest kierowany.
	\item[Specyfikacja wewnętrzna] Szczegółowo przedstawia implementację systemu, omawia zastosowane algorytmy, strukturę kodu i kluczowe rozwiązania techniczne.
	\item[Weryfikacja i walidacja] Opisuje sposób testowania systemu, analizuje wykryte błędy oraz wprowadzone poprawki, a także ocenia skuteczność działania rozwiązania.
	\item[Podsumowanie i wnioski] Podsumowuje wyniki pracy, omawia osiągnięte cele oraz wskazuje możliwe kierunki dalszego rozwoju systemu.
\end{description}

%--------------------------------------
%
%
% TODO: Dodać zdjęcia???, tytuł tematu???, krótkie wprowadzenie na początku każdej sekcji/rozdziału
%
% Status: 99.98%
%--------------------------------------
\chapter{Śledzenie ruchu gałek ocznych -- analiza tematu}
\label{ch:analiza-tematu}

\section{Sformułowanie problemu}
\label{sec:sformulowanie-problemu}

Widzeniem plamkowym (fovealnym) nazywamy wspomniane już wcześniej zjawisko, że widzimy w największej rozdzielczości jedynie centrum pola widzenia. Wynika to z budowy siatkówki, która posiada plamkę żółtą (fovea) -- jest to niewielki obszar, który charakteryzuje się największym zagęszczeniem czopków. Taka budowa siatkówki sprawia, że nieustannie poruszamy oczami. Skupiając wzrok na danym punkcie, musimy być gotowi na to, że cała reszta pola widzenia straci dla nas na ostrości. Jest to proces, który przypomina filtracje wielu otaczających nas informacji, do paru, które mają w tym momencie znaczenie.

\subsection{Uwaga wzrokowa}
\label{subsec:uwaga-wzrokowa}

Uwaga wzrokowa może zostać opisana przez idiomy ,,gdzie'' i ,,co'', które pozwalają na zobrazowanie jej selektywnej natury. ,,Gdzie'' to proces wizualnego wyszukiwania i wyboru lokalizacji, która zwróciła naszą uwagę w celu dokładniejszego zbadania. Istotnym aspektem tego procesu selekcji jest widzenie peryferyjne, czyli takie które obejmuje obszar oddalony od naszego punktu skupienia, ale wciąż pozwalający na wyodrębnienie kształtów i ruchu, co w pewien sposób prowadzi nasze centralne spojrzenie. Przykładem może być spojrzenie przez okno -- w pierwszym momencie nasz wzrok kierowany jest na wyraźne kształty, światła czy nagłe ruchy jak np. lądującego ptaka. 

,,Co'' można nazwać odwrotnością ,,gdzie'' -- jest to proces szczegółowego badania danego obszaru. Charakteryzują go takie zjawiska jak fiksacja, czyli stabilizacja wzroku na danym punkcie oraz ruchy sakadowe, czyli szybkie mimowolne ruchy oczu pomiędzy kolejnymi punktami, które pozwalają na obserwację. Całość tworzy kanał percepcyjny o ograniczonym zasięgu przestrzennym, obejmujący interesujący obszar i budując w ten sposób naszą świadomość oraz zrozumienie celu naszego spojrzenia. Wracając do poprzedniego przykładu -- po zauważeniu ptaka, nasze spojrzenie skupi się na nim (fiksacja), a ruchy sakadowe oczu pozwolą na dokładne zbadanie jego kształtu, koloru i detali. To właśnie połączenie ,,gdzie'' i ,,co'' pozwala na głębsze zrozumienie otaczającego nas świata. Owe ścieżki skanowania dają nam 
wgląd w proces poznawczy badanej osoby, jako że pomiar widzenia fovealnego w czasie odzwierciedla chwilowe i jawne skupienie uwagi wzrokowej obserwatora. 

\subsection{Poszukiwanie wzrokowe}
\label{subsec:poszukiwanie-wzrokowe}

Warto wspomnieć o poszukiwaniu wzrokowym. Jest to proces aktywnego przeszukiwania pola widzenia w celu znalezienia konkretnego celu, a częścią tego procesu jest etap przeduwagowy. Etap ten równolegle analizuje duży obszar widzenia jednocześnie. Dzieje się to automatycznie, nieświadomie i nie wymaga widzenia fovealnego. To, co charakteryzuje ten proces to umiejętność rozpoznania czterech podstawowych cech: koloru, rozmiaru, orientacji oraz obecności i/lub kierunku ruchu. Etap przeduwagowy jest pierwszym krokiem poszukiwania wzrokowego i obejmuje dużą część poszukiwania typu równoległego (np. próba zauważenia nagłego ruchu spadającej gwiazdy) oraz relatywnie niewielką część poszukiwania typu seryjnego (np. przeszukiwanie obiektów na stole w poszukiwaniu kluczy), które wymaga uwagi oraz przenoszenia wzroku od obiektu do obiektu. 

\subsection{Analiza ruchów oczu}
\label{subsec:analiza-ruchow-oczu}

Rozważania omówione w podrozdziale \ref{sec:sformulowanie-problemu} pozwalają przejść do głównego nurtu tematu śledzenia ruchu gałek ocznych, czyli analizy ruchów oczu. Rozróżniamy pięć podstawowych typów ruchów oczu: wspomniane już wcześniej sakadyczne, wergencyjne, przedsionkowe, płynne podążanie (smooth pursuit) oraz oczopląs fizjologiczny, który jest naturalny dla zdrowej osoby i często występuje podczas fiksacji. Ruchy te można podzielić na dobrowolne, mimowolne i odruchowe. Sygnały je kontrolujące pochodzą z obszarów korowych mózgu. Do opisania tych procesów można posłużyć się modelowaniem matematycznym. Do zrozumienia jawnej uwagi wzrokowej wystarczy modelowanie trzech typów ruchów: fiksacji, która pokazuje chęć utrzymania wzroku na stacjonarnym obiekcie; sakad, które mogą wskazywać na chęć zmiany punktu uwagi; oraz płynnych pościgów, które -- podobnie do fiksacji -- pozwalają na śledzenie obiektu, ale ruchomego. 

\subsubsection{Ruchy sakadowe}
\label{subsubsec:ruchy-sakadowe}

Sygnał sakad można opisać jako funkcję impuls/skok, gdzie impuls na wejściu reprezentuje prędkość, a skok -- pozycję. Impuls jest przepuszczany przez filtr, który przekształca go w skok. Prostą reprezentacją ruchu sakadowego jest filtr liniowy różniczkujący, który dokonuje potrzebnej konwersji informacji z prędkości na przemieszczenie. Wzór owego filtra w dziedzinie czasu można zapisać równaniem \ref{eq:sakady-filtr}.

\begin{align}
	x_t &= g_0 \cdot s_t + g_1 \cdot s_{t-1} + g_2 \cdot s_{t-2} + \ldots
	\label{eq:sakady-filtr}
\end{align}

Wyjście $x_t$ zależy od bieżącego wejścia $s_t$ oraz jego poprzednich wartości $(s_{t-1}, s_{t-2}, \ldots)$ ważonych odpowiednimi współczynnikami filtra $(g_0, g_1, g_2, \ldots)$. Wzór \ref{eq:sakady-filtr} można przedstawić sumarycznie równaniem \ref{eq:sakady-sumaryczny}.

\begin{align}
	x_t &= \sum_{k=0}^{\infty} g_k \cdot s_{t-k}
	\label{eq:sakady-sumaryczny}
\end{align}

Filtr Haara jest jednym z przykładów filtrów, które przybliżają różniczkowanie. Jest to filtr o długości 2, czyli operujący jedynie na dwóch kolejnych próbkach sygnału wejściowego $(s_t, s_{t-1})$. Przybliża on pierwszą pochodną, przyjmując, że prędkość zmian sygnału jest stała w czasie trwania dwóch próbek. Współczynniki filtra są równe $g_0 = 1$ oraz $g_1 = -1$, co oznacza, żże różniczkowanie jest realizowane poprzez różnicę między dwiema kolejnymi próbkami sygnału. W związku z tym transmitancję filtru Haara można zapisać jako równanie \ref{eq:sakady-transmitancja}.

\begin{align} %!!!!!!!!!!!!!!!!!!!!!!!!!!!!TUTAJ JEST COŚ ŹLE Z TYM RÓWNANIEM%!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	x_t &= g_0 \cdot s_t + g_1 \cdot s_{t-1} \nonumber \\
	x_t &= 1 \cdot s_t - 1 \cdot s_{t-1} \nonumber \\
	x_t &= s_t - s_{t-1} \nonumber \\
	\mathcal{Z}\{x_t\} &= \mathcal{Z} \{s_t - s_{t-1}\} \nonumber \\
	X(z) &= (1 - z) \cdot S(z) \nonumber \\ % POWINNO BYĆ CHYBA (1 - z^{-1}) \cdot S(z)
	\frac{X(z)}{S(z)} &= 1 - z \label{eq:sakady-transmitancja}
\end{align}   %!!!!!!!!!!!!!!!!!!!!!!!!!!!!TUTAJ JEST COŚ ŹLE Z TYM RÓWNANIEM%!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Diagram omawianego modelu przedstawiony jest na rysunku \ref{fig:model-sakad}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{pic/modele/model_sakad.png}
	\caption{Diagram modelu ruchów sakadowych z filtrem liniowym różniczkującym.}
	\label{fig:model-sakad}
\end{figure}

\subsubsection{Płynne podążanie}
\label{subsubsec:plynne-podazanie}

Płynne podążanie występuje, gdy obserwator śledzi obiekt w ruchu. Ruch ten, oczywiście, nie może być zbyt gwałtowny. Tego typu śledzenie jest przykładem systemu sterowania z ujemnym sprzężeniem zwrotnym. Do modelowania owych ruchów używana jest prosta pętla, którą można opisać następującym równaniem \ref{eq:plynne-podazanie-pentla} w dziedzinie czasu.

\begin{align}
	h \cdot (s_t - x_t) &= x_{t+1} 
	\label{eq:plynne-podazanie-pentla}
\end{align}

Wejście $s_t$ to pozycja celu, wyjście $x_t$ to pożądana pozycja oka. Można zauważyć, że w tym przypadku nie jest wymagana transformacja informacji wejściowej na wyjściową, a jedynie zmodyfikowanie jej wartości. W związku z tym $h$ jest liniowym, niezmiennym w czasie filtrem, czyli po prostu wzmocnieniem systemu. Przeprowadźmy teraz transformację $\mathcal{Z}$ na równaniu \ref{eq:plynne-podazanie-pentla} aby uzyskać transmitancję systemu przedstawioną wzorem \ref{eq:plynne-podazanie-transmitancja}.

\begin{align} %!!!!!!!!!!!!!!!!!!!!!!!!!!!!TUTAJ JEST COŚ ŹLE Z TYM RÓWNANIEM%!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	\mathcal{Z}\{h \cdot (s_t - x_t)\} &= \mathcal{Z}\{x_{t+1}\} \nonumber \\
	H(z) \cdot (S(z) - X(z)) &=  X(z) \nonumber \\ % POWINNO BYĆ CHYBA z \cdot X(z)
	H(z) \cdot S(z) - H(z) \cdot X(z) &=  X(z) \nonumber \\
	H(z) \cdot S(z) &= (H(z) + 1) \cdot X(z) \nonumber \\
	\frac{X(z)}{S(z)} &= \frac{H(z)}{H(z) + 1} \label{eq:plynne-podazanie-transmitancja}
\end{align}   %!!!!!!!!!!!!!!!!!!!!!!!!!!!!TUTAJ JEST COŚ ŹLE Z TYM RÓWNANIEM%!!!!!!!!!!!!!!!!!!!!!!!!!!!!

W ten sposób sygnał z receptorów oka służy za błąd, który następnie jest kompensowany w celu utrzymania obrazu w przestrzeni widzenia plamkowego. Diagram modelu przedstawiony jest na rysunku \ref{fig:model-smooth}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/modele/model_smooth.png}
	\caption{Diagram modelu płynnego podążania z liniowym sprzężeniem zwrotnym.}
	\label{fig:model-smooth}
\end{figure}

\subsubsection{Fiksacja}
\label{subsubsec:fiksacja}

Można zauważyć, że fiksacja jest procesem podobnym do płynnego podążania, z tą różnicą, że w tym przypadku obiekt jest nieruchomy. Jednakże proces ten nie da się bezpośrednio z sobą porównać i najprawdopodobniej nie ma wspólnego obwodu neuronalnego. Nasze komórki systemu wzrokowego są fizjologicznie wrażliwe na ruch, gdyby dany obiekt został unieruchomiony względem siatkówki, po krótkim czasie widzenie zaniknie. Powoduje to konieczność mikrosakad i innych mimowolnych, drobnych ruchów oczu. Można więc uznać, że model fiksacji jest podobny do modelu płynnego podążania, który próbuje utrzymać pozycję oka na danym punkcie, a mikrosakady i inne ruchy można uznać za szum w systemie kontrolnym, który można wyrazić jako $e_t = x_t - s_t$. Jest to losowa fluktuacja wokół punktu fiksacji, a jej wartość średnia pozostaje stała.

Ruch gałek ocznych i związane z nim widzenie jest bardzo szerokim tematem interdyscyplinarnym, na potrzeby tego projektu sformułowanie problemu w podrozdziale \ref{sec:sformulowanie-problemu}, chociaż uproszczone i nie wyczerpujące, pozwala na zrozumienie w pełni istoty prezentowanego systemu.

% Osadzenie tematu w kontekście aktualnego stanu wiedzy

\section{Osadzenie tematu w kontekście aktualnego stanu wiedzy}
\label{sec:osadzenie-tematu-w-kontekscie-aktualnego-stanu-wiedzy}

Pomiar ruchu oczu był realizowany już w latach 70. XX wieku. Nic dziwnego więc, że od tego momentu powstały różne techniki pozwalające na zebranie owych danych. Należy podkreślić, że metody te można podzielić na te mierzące ruch oczu względem głowy oraz takie, które mierzą orientację oczu w przestrzeni, czyli ,,punkt spojrzenia''.

\subsection{Metody śledzenia ruchu oczu: przegląd historyczny i alternatywy}
\label{subsec:metody-sledzenia-ruchu-oczu-przeglad-historyczny-i-alternatywy}

Najstarszą z metod śledzenia ruchu oczu jest elektrookulografia (EOG ang. \english{electrooculography}), wciąż wykorzystywana np. w badaniach klinicznych. Metoda ta polega na umieszczeniu elektrod na skórze twarzy wokół oczu i pomiarze różnicy potencjałów. Z założenia mierzy ona ruch oczu względem głowy, ale można ją rozszerzyć o pomiar ruchu głowy, dając możliwość wyliczenia punktu spojrzenia. 

Jedna z najdokładniejszych metod pomiaru ruchu oczu polega na umieszczeniu soczewki kontaktowej bezpośrednio na gałce ocznej. Soczewka ta powinna być odpowiednio duża, by objąć zarówno rogówkę, jak i twardówkę, a na jej powierzchni umieszczany jest albo obiekt optyczny, którego zadaniem jest dokładne odbijanie światła lub dostarczenie wyraźnych kształtów potrzebnych do śledzenia, albo cewka wykonana z drutu, która, poruszając się w polu elektromagnetycznym, pozwala na wykonanie pomiaru. Metoda ta jest bardzo dokładna, ale niezwykle inwazyjna i nieprzyjemna dla badanego. Podobnie jak elektrookulografia, mierzy ruch oczu względem głowy.

Wideo-okulografia i foto-okulografia tworzą razem szeroką grupę metod, które opierają się na analizie wyróżniających się cech oka podczas jego obrotu, takich jak kształt źrenicy, pozycja granicy tęczówki i twardówki czy odbicie światła (często podczerwonego) od rogówki. Metody te są nieinwazyjne, ale same w sobie nie pozwalają na określenie punktu spojrzenia. Dlatego w celu jego wyznaczenia często stosuje się unieruchomienie głowy osoby badanej, wyznaczenie punktu odniesienia (np. przez odbicia światła od powierzchni oka) lub odpowiednią kalibrację, np. prosząc osobę badaną o utrzymanie wzroku na danym punkcie ekranu.

\subsection{Analiza wideo z wykorzystaniem źrenicy i odbicia rogówkowego}
\label{subsec:analiza-wideo-z-wykorzystaniem-zrenicy-i-odbicia-rogowkowego}

Opisane metody wymagają unieruchomienia głowy (przez różnego rodzaju podpórki pod brodę lub głowę albo nawet belki nagryzowe) lub zastosowania dodatkowego sprzętu mierzącego ruch głowy, by wyznaczyć punkt spojrzenia. Jest to główna wada tych metod, ponieważ miejsce skupienia uwagi daje istotne informacje, które są najczęściej pożądane przez użytkowników okulografów. Dlatego metoda należąca do grupy wideo-okulografii, opierająca się na analizie obrazu wideo z wykorzystaniem wykrycia źrenicy i odbicia rogówkowego, pozwalająca na określenie punktu spojrzenia z dużą dokładnością, jest jedną z najpopularniejszych zarówno w świecie naukowym, jak i komercyjnym. System prezentowany w tej pracy można zaliczyć do wideo-okulografii z możliwością aktywowania trybu pomiaru odbicia rogówkowego, dlatego metoda ta zostanie omówiona szerzej od pozostałych.

Wykrywanie punktu spojrzenia metodą detekcji źrenicy i odbicia rogówkowego wymaga kilku kluczowych elementów. Potrzebna jest kamera wideo, która zarejestruje aktualny stan gałek ocznych, oraz sprzęt, który pozwoli na przetworzenie obrazu i ostatecznie wykrycie interesujących cech, najlepiej w czasie rzeczywistym. Obecnie oba wymienione elementy stają się coraz tańsze i bardziej wydajne. W rzeczywistości większość osób posiada je już w swoim smartfonie, co sprawia, że technika ta staje się bardziej dostępna dla szerokiego grona użytkowników. Aparatura ta jest także coraz bardziej miniaturyzowana, dzięki czemu aktualnie dostępne są zarówno systemy montowane na głowie, jak i montowane na biurku. Działają one na tej samej zasadzie, różniąc się zasadniczo jedynie wielkością.

Warto jednak zaznaczyć, że systemy montowane na głowie zazwyczaj posiadają także dodatkową kamerę wyznaczającą tzw. POV (ang. \english{point of view} -- punkt widzenia), a spojrzenie jest monitorowane względem tego obrazu. Natomiast systemy montowane na biurku zazwyczaj mierzą spojrzenie względem pewnej powierzchni, np. monitora.

Należy omówić także źródło światła, którego odbicie rejestrowane jest podczas detekcji. Możliwe jest wykorzystanie punktowego źródła światła białego o odpowiednim natężeniu, jednakże może to prowadzić do dyskomfortu przy użytkowaniu lub nawet chwilowego oślepienia osoby badanej, wytrącając ją ze stanu skupienia. W związku z tym najczęściej wykorzystywane są źródła światła podczerwonego, które znajdują się poza widzialnym spektrum, nie irytując oka, nie zakłócając procesu badania i pozwalając na użycie wyższego natężenia. Operowanie na takim zakresie częstotliwości światła wymaga jednak użycia specjalnych kamer zdolnych do rejestrowania światła IR (ang. \english{infrared}).

Omawiane odbicie rogówkowe nazywane jest obrazem Purkinjego. Budowa oka sprawia, że pojawiają się cztery odbicia: pierwsze to odbicie od zewnętrznej powierzchni rogówki (najbardziej widoczne), drugie -- od wewnętrznej powierzchni rogówki, trzecie -- od zewnętrznej powierzchni soczewki, a czwarte -- od wewnętrznej powierzchni soczewki. Aby poprawnie wyznaczyć punkt spojrzenia, wymagane jest określenie dwóch punktów odniesienia: pierwszego, określającego obrót oczu w oczodole, oraz drugiego, wskazującego stałe położenie względem oczu. Pierwszym punktem zazwyczaj jest środek źrenicy, a drugim -- najczęściej pierwszy obraz Purkinjego, powstały przez odbicie światła ustabilizowanego względem głowy lub powierzchni badanej.

Przy nieruchomej głowie ruch oczu będzie odczytywany jako zmieniająca się różnica pomiędzy tymi dwoma punktami. W przeciwnym przypadku, gdy głowa poruszy się, a oczy pozostaną w fiksacji na danym punkcie, dla systemu montowanego na biurku różnica ta pozostanie stała, natomiast dla systemu montowanego na głowie różnica będzie zmieniać się proporcjonalnie do zmiany punktu widzenia.

Istnieją także okulografy piątej generacji z technologią DPI (ang. \english{Dual Purkinje Image}). Mierzą one dodatkowo czwarty obraz Purkinjego, dzięki czemu mogą rozróżniać ruchy translacyjne oka od ruchów rotacyjnych. Przy translacji obrazy Purkinjego poruszają się o tę samą odległość, natomiast przy rotacji zmieniają swoje rozdzielenie, co zwiększa precyzję pomiaru. Wymaga to jednak bardziej skomplikowanego oprogramowania, a także unieruchomienie głowy może okazać się konieczne.

\section{Studia literaturowe}
\label{sec:studia-literaturowe}

% UWAGA - Czy cytat nie powinien być przeniesiony na koniec akapitu?
Sformułowanie problemu i jego osadzenie w kontekście aktualnego stanu wiedzy w podrozdziałach \ref{sec:sformulowanie-problemu} i \ref{sec:osadzenie-tematu-w-kontekscie-aktualnego-stanu-wiedzy} zostało przedstawione w ścisłym oparciu o książkę ,,Eye Tracking Methodology: Theory and Practice'' autorstwa Andrew T. Duchowskiego \cite{bib:eye-tracking-methodology}. Treść tych podrozdziałów stanowi opracowanie wiedzy zawartej w tej pozycji, a jej autor jest uznanym specjalistą w zakresie śledzenia ruchu oczu. Książka ta dogłębnie omawia zarówno podstawowe procesy wzrokowe w różnych kontekstach naukowych, jak i szeroko przedstawia praktyczne aspekty pomiaru ruchu oczu. Część teoretyczna rozdziału \ref{ch:analiza-tematu} skupia się jedynie na najistotniejszych aspektach, niezbędnych do zrozumienie istoty prezentowanego systemu, a czytelnik zainteresowany pogłębieniem wiedzy w tym zakresie może odwołać się do wspomnianej książki.

Eye tracking staje się coraz bardziej dostępny, co sprawia, że technologia ta znajduje zastosowanie w wielu obszarach, zarówno w badaniach naukowych jak i do użytku konsumenckiego czy komercyjnego. Część producentów owych systemów udostępnia publikacje naukowe, w których zostały one wykorzystane, a ich analiza przedstawia różne rozwiązania omawianego tematu.

\subsection{Metoda źrenicy i odbicia rogówkowego w badaniach}
\label{subsec:metoda-zrenicy-i-odbicia-rogowkowego-w-badaniach}

Jednym z przedstawicieli firm skupiających się na analizie zachowań ludzkich jest iMotions, który wyróżnia się modularnym systemem z centralnym hubem. Firma ta oferuje biosensory w formie modułów, pozwalające np. na śledzenie oczu, analizę wyrazu twarzy, rejestrowanie aktywności elektrycznej mózgu oraz rejestrowanie aktywności serca.

Dane zebrane przez różne czujniki są integrowane w jednym oprogramowaniu, co pozwala badaczom na prostą synchronizację i analizę danych w jednym systemie \cite{bib:iMotions-about-us}. Jednak mimo tak obszernego asortymentu biosensorów, to właśnie moduł śledzenia wzroku na ekranie (montowany na biurku) był wykorzystywany najczęściej w badaniach naukowych, przy czym eye tracking był najczęściej stosowany w systemach jednomodułowych \cite{bib:iMotions-2023-report}. Pokazuje to, jak uniwersalnym i podstawowym narzędziem jest eye tracker w badaniach behawioralnych oraz nie tylko.

Jednym z przeprowadzonych badań z użyciem technologii iMotions było ,,\english{Using Psychophysiological Data to Facilitate Reflective Conversations with Children about their Player Experiences}''. Podobne badania były przeprowadzane wielokrotnie w stosunku do osób dorosłych, jednak w przypadku grupy badanej składającej się z dzieci badacze mogą napotkać znaczny problem z odpowiednim zrozumieniem wywiadu, w którym uczestnicy dzielą się swoimi wrażeniami z przeprowadzonej rozgrywki. Omawiane badanie miało głównie na celu sprawdzenie, czy zebrane dane z biosensorów są w stanie wspomóc refleksję dzieci na temat ich doświadczenia w grze oraz czy dzieci napotkają problemy z interpretacją prezentowanych danych, ponieważ odnoszenie się do stanu psychofizjologicznego może zminimalizować problem komunikacyjny pomiędzy dzieckiem, które posiada ograniczone umiejętności werbalne, a badaczem.

Do śledzenia ruchu gałek ocznych na ekranie zastosowano okulograf Smart Eye AI-X o częstotliwości próbkowania $60 \ \unit{\hertz}$, posługujący się techniką detekcji źrenic i odbicia rogówkowego, co pozwalało na sporą swobodę ruchu głowy w przestrzeni $35 \, \unit{\centi\metre} \times 30 \, \unit{\centi\metre}$. Jednocześnie posiadał dokładność (różnica pomiędzy rzeczywistą pozycją spojrzenia a pozycją spojrzenia zarejestrowaną przez okulograf) $\ang{0.5}$ i precyzję (średnia kwadratowa punktów mierzonych w jednej pozycji oka) $\ang{0.1}$, zwracając dane wyjściowe binokularowe z wskaźnikiem jakości, zawierające: punkt spojrzenia, średnicę źrenicy oraz znacznik czasowy \cite{bib:iMotions-smart-eye-ai-x}. Oprócz tego zastosowano moduł rejestrujący aktywność sercową, analizujący wyrazy twarzy oraz mierzący reakcję skórno-galwaniczną.

Badanie składało się z dwóch etapów. Najpierw zebrano dane w laboratorium, gdzie badane dzieci grały w dwie gry oraz uczestniczyły w wywiadach bezpośrednio po zakończonej rozgrywce. Następnie prezentowano badanym momenty z pierwszego etapu, uwzględniając dane z biosensorów, i zadawano pytania o ich doświadczenia.

Badania wskazały, że dzieci są w stanie zrozumieć i odpowiednio odnieść się do prezentowanych danych psychofizjologicznych, a oparcie się na nich może pomóc w zwerbalizowaniu swojego doświadczenia oraz informacji zwrotnej przez badane dzieci. Dzieci nie miały problemu ze zrozumieniem większości pomiarów, z wyjątkiem reakcji skórno-galwanicznej. Jednakże dane eye-trackingowe były swego rodzaju wyjątkiem, ponieważ nie prezentowały bezpośrednio doświadczenia ani uczuć związanych z danym wydarzeniem w grze, lecz raczej dawały wgląd w strategię obraną przez dziecko zmagające się z aktualnym wyzwaniem.

Mimo tego prezentowane śledzenie punktu spojrzenia było naturalne do interpretacji przez badanych \cite{bib:iMotions-children}. Publikacja ta pokazuje, jak okulografia może zostać skutecznie użyta w badaniach naukowych, ale przede wszystkim przedstawia istotność tej techniki w zastosowaniach komercyjnych, np. w testowaniu doświadczeń graczy przy produkcji gier wideo dla różnych grup wiekowych.

\subsection{Porównanie z video-okulografią z użyciem kamerki internetowej}
\label{subsec:porownanie-z-video-okulografia-z-uzyciem-kamerki-internetowej}

Warto omówić także przykłady systemów opierających się jedynie na analizie wideo z kamerki internetowej, jako że system prezentowany w tej pracy działa przede wszystkim na tej zasadzie. Ten typ okulografii, opartej na analizie wideo w czasie rzeczywistym, cieszy się dużą popularnością wśród konsumentów, jako że sprzętowo wymaga od użytkownika posiadania jedynie komputera z wbudowaną lub zewnętrzną kamerą, a całość przetwarzania obrazu, wyświetlania i zbierania danych oraz wyliczania punktu spojrzenia realizowana jest przez software producenta eye trackera.

Istotność takiego rozwiązania pojawia się także, gdy przeprowadza się badania na dużej grupie, jako że osoba badana sama dostarcza sprzęt w formie laptopa czy nawet smartfona, a zbierane dane przesyłane są do badaczy przez internet, eliminując potrzebę stwarzania przestrzeni laboratoryjnej i planowania dogodnych terminów eksperymentów.

Dlatego firmy specjalizujące się w okulografii często oferują także oprogramowanie do śledzenia ruchu gałek ocznych przez kamerkę internetową, jak wspomniana już wcześniej firma iMotions. Często dostępne są także rozwiązania od niezależnych dostawców oprogramowania, które za subskrypcją, opłatę lub całkowicie bezpłatnie oferują śledzenie punktu spojrzenia przez kamerę komputera. Jednym z takich programów, który zostanie omówiony, jest darmowy GazeRecorder.

W publikacji ,,\english{WebET 3.0 -Validation Study Report}'' przeprowadzono badania sprawdzające efektywność śledzenia ruchu gałek ocznych przez oprogramowanie WebET 3.0 firmy iMotions z wykorzystaniem kamerek internetowych uczestników, bez względu na ich oświetlenie, rozdzielczość kamery czy prędkość internetu. Grupa badana została utworzona tak, by reprezentować różne pochodzenie, wiek, płeć i kolor oczu. Uczestnicy mogli także posiadać owłosienie twarzy oraz nosić okulary.

By stworzyć punkty odniesienia, pokazano badanym gify z kotami oraz emotikony w tych samych miejscach, a następnie obliczono dokładność, mierząc odległości pomiędzy zarejestrowanymi punktami spojrzenia a ustalonymi punktami odniesienia.

Wyniki dokładności wskazały, że system iMotions jest niezależny od zmiennych wynikających z pochodzenia, koloru oczu, wieku, płci oraz owłosienia twarzy. Jednak posiadanie okularów mogło wpływać na wynik śledzenia. Rodzaj i natężenie oświetlenia także nie wpływały znacząco na działanie WebET 3.0. Ponad $90\%$ uczestników posiadało dokładność mniejszą od $\ang{5}$ dokładności, $70\%$ badanych posiadało dokładność mniejszą od $\ang{3}$, a mediana dokładności wynosiła $\ang{2.08}$ \cite{bib:iMotions-WebET}. Dokładność alternatywnego darmowego oprogramowania GazeRecorder została zaprezentowana w przeglądzie systematycznym i wynosiła $\ang{1,43}$, z możliwością poprawy do $\ang{1,3}$ przez unieruchomienie głowy, pozwalając na noszenie okularów, za wyjątkiem okularów antyrefleksyjnych \cite{bib:GazeRecorder-Review}. 

\subsection{Podsumowanie metod w literaturze}
\label{subsec:Podsumowanie-metod-w-literaturze}

Metoda śledzenia ruchu gałek ocznych przy użyciu źrenicy i odbicia rogówkowego w podczerwieni znacznie przewyższa -- pod względem dokładności -- metodę polegającą na kamerce internetowej. W związku z tym, przy badaniach skupiających się na analizie gęsto ułożonych informacji, wyspecjalizowany okulograf może być bardziej pożądany. Jednakże aktualna dostępność sprzętu i oprogramowania potrzebnego do śledzenia z użyciem kamerki internetowej daje możliwość bardziej obszernych zastosowań, a ich dokładność pozwala na efektywne śledzenie punktu spojrzenia w przypadkach, gdy informacje są ułożone z odpowiednimi dystansami.






%%%%%%%%%%%%%%%%%%%%%%%%





%--------------------------------------
%
%
% TODO: krótkie wprowadzenie na początku każdej sekcji/rozdziału
%
% Status: 99%
%--------------------------------------
\chapter{Wymagania i narzędzia}
\label{ch:wymagania-i-narzedzia}

\section{Wymagania funkcjonalne}
\label{sec:wymagania-funkcjonalne}

Postawienie konkretnych granic funkcjonalnych projektu jest kluczowym elementem tworzenia rozwiązania. Działają one na zasadzie zabezpieczenia przed usprawnianiem i rozszerzaniem projektu bez końca, a także pozwalają na skupienie uwagi inżyniera na istotnych aspektach. Można zauważyć korelację pomiędzy rolą wymagań funkcjonalnych w tworzeniu projektu a plamką żółtą w oku, omawianą w podrozdziale \ref{sec:sformulowanie-problemu}. Oba elementy dbają o to, by wiele informacji zostało przefiltrowanych do tych, które są najważniejsze.

\subsection{Limitacje sprzętowe i środowiskowe}
\label{subsec:Limitacje-sprzetowe-i-srodowiskowe}

Do śledzenia ruchu gałek ocznych używany jest bardzo szeroki wachlarz sprzętów i związanych z nimi rozwiązań technicznych, omówionych w podrozdziałach \ref{sec:osadzenie-tematu-w-kontekscie-aktualnego-stanu-wiedzy} i \ref{sec:studia-literaturowe}. Jednakże w założeniu tego projektu było oparcie się na systemie wizyjnym, co pozwala na ograniczenie znanych metod do video-okulografii. Kolejną ustaloną limitacją było opieranie się jedynie na aktualnie dostępnym sprzęcie, którym był laptop ASUS Vivobook 15 z wbudowaną kamerką internetową USB2.0 HD UVC o częstotliwości próbkowania $30 \ \unit{\hertz}$. Podczas korzystania z kamery automatycznie włączane było także niewielkie źródło światła, przylegające z jej prawej strony.

Tego typu laptop z wbudowaną kamerą reprezentuje jakość sprzętu często używaną do zastosowań biurowych i użytku własnego, dlatego też założono, że źródła światła powinny zostać ograniczone do podstawowego oświetlenia wewnętrznego, tworząc środowisko typowe dla pokoju mieszkalnego, w którym korzysta się z komputera.

\subsection{Wykorzystanie platform i narzędzi programistycznych}
\label{subsec:Wykorzystanie-platform-i-narzedzi-programistycznych}

By uprościć użycie komputera (wspomnianego w punkcie \ref{subsec:Limitacje-sprzetowe-i-srodowiskowe}), zdecydowano się na pozostanie przy już zainstalowanym systemie operacyjnym, którym jest Windows 11.

Podobnie postawiono wymaganie względem edytora kodu źródłowego i systemu kontroli wersji, jako że użycie znajomego środowiska, które jest już zainstalowane i było wcześniej używane na wykorzystanym laptopie, pozwala na szybkie zamknięcie przygotowań do pracy. W związku z tym skupiono się na rozwiązaniach, które są kompatybilne z edytorem tekstu Visual Studio Code i klientem Git z graficznym interfejsem użytkownika GitHub Desktop, jako że narzędzia te ściśle współpracują ze sobą i serwisem hostingowym GitHub, tworząc intuicyjną i znajomą w użyciu całość.

Stawianym wymaganiem wobec języka programowania, oprócz wspomnianej kompatybilności, jest jego popularność, uniwersalność, klarowność oraz szeroki wybór bibliotek. Język o takich cechach znacznie zwiększy produktywność, uprości stosowanie bardziej skomplikowanych narzędzi i pozwoli na efektywne uczenie się oraz rozwiązywanie problemów, opierając się na społeczności użytkowników. Zalety te wspomogą uzyskanie natychmiastowych rezultatów pracy oraz skoncentrowanie uwagi na poprawie i rozwoju funkcjonalności systemu.

Łatwo dostępnym i popularnym językiem o wysokim poziomie abstrakcji jest Python, który jest oficjalnie wspierany przez Visual Studio Code. Język ten umożliwia korzystanie z biblioteki OpenCV, która daje dostęp do wielu rozwiązań przetwarzających obraz. Ostatnim, bardziej personalnym atutem proponowanego języka jest niewielkie ówczesne doświadczenie w programowaniu w nim, a co za tym idzie -- szansa na zdobycie cennych umiejętności w trakcie pracy.

\subsection{Zakres działania systemu}
\label{subsec:zakres-dzialania-systemu}

Nakreślone limitacje w punkcie \ref{subsec:Limitacje-sprzetowe-i-srodowiskowe} i \ref{subsec:Wykorzystanie-platform-i-narzedzi-programistycznych} odpowiednio ograniczyły możliwe rozwiązania, pozwalając na ustalenie realnych funkcjonalności programu. Aspektem, który przede wszystkim determinuje stopień skomplikowania stawianych wymagań, jest ograniczony czas na wykonanie omawianego systemu. W związku z tym rozwiązaniem, które może zapewnić użyteczność planowanego systemu -- nawet w przypadku jego niepełnego ukończenia -- jest skoncentrowanie się na podstawowych funkcjonalnościach. Dzięki temu powstanie solidny szkielet projektu, oferujący funkcje dostępne na danym etapie prac.

W konsekwencji zakres działania systemu śledzącego ruch oczu przedstawia się następująco:

\begin{enumerate}
	\item Przetwarzanie obrazu z kamery powinno odbywać się w czasie rzeczywistym. Dzięki takiemu rozwiązaniu nie będzie potrzebne nagrywanie, a także pozwoli to na weryfikację pracy systemu w trakcie użytkowania. Biblioteka OpenCV posiada wiele rozwiązań, które pozwalają na operowanie na obrazie w czasie rzeczywistym.
	\item Proces śledzenia ruchu gałek ocznych powinien przebiegać poprzez wykrycie twarzy, następnie w rejonie twarzy przeprowadzenie detekcji oczu, a dopiero w rejonie oczu -- wykrycie źrenicy. Dzięki takiemu założeniu obraz będzie stopniowo ograniczany do rejonu zainteresowania, co zagwarantuje ograniczenie błędnych detekcji.
	\item System powinien działać w sposób binokularny, pozwalając na wykrycie i zidentyfikowanie obu oczu (a następnie źrenic) z rozróżnieniem lewego i prawego oczodołu. Dzięki takiemu rozwiązaniu dane wyliczane przez system będą skategoryzowane, co zabezpieczy przed błędami i niespójnościami w analizie.
	\item Wykrywanie źrenic powinno być uniwersalne względem oświetlenia otoczenia. Oczywistym jest, że skrajnie złe oświetlenie uniemożliwi poprawne działanie, ale program powinien cechować się pewnym stopniem elastyczności, nie wymuszając użytkowania w jednym konkretnym środowisku.
	\item Aby obliczyć przemieszczenie źrenicy, musi zostać wyznaczony umowny punkt odniesienia względem powierzchni, na której skupiona jest uwaga. Punkt ten powinien być odporny na niewielkie ruchy głowy. Dodatkowym atutem będzie możliwość prostej kalibracji owego punktu. Jest to kluczowy element pozwalający na detekcję ruchu oczu.
	\item Wyznaczanie ruchu oczu jedynie poprzez umowny punkt odniesienia, o ograniczonej możliwości weryfikacji poprawności pomiaru, nie jest rozwiązaniem idealnym i może być niewystarczające dla bardziej zaawansowanych użytkowników. W związku z tym system powinien oferować opcję wyznaczenia ruchu gałek ocznych względem realnego punktu odniesienia, np. poprzez zastosowanie dodatkowego sprzętu.
	\item Wyliczone przemieszczenie źrenicy w czasie rzeczywistym powinno zostać zapisane do pliku, który pozwoli na analizę zebranych danych, w tym pozycji źrenicy, indeksu wskazującego przynależność do odpowiedniego oka oraz momentu czasowego zebranego pomiaru.
\end{enumerate}


\section{Wymagania niefunkcjonalne}
\label{sec:wymagania-niefunkcjonalne}

Jak ustalono w punkcie \ref{subsec:zakres-dzialania-systemu}, projekt ten ma skupić się na utworzeniu solidnego szkieletu programu. Takie rozwiązanie kładzie nacisk przede wszystkim na funkcjonalność i wymusza jednocześnie zastosowanie jedynie najważniejszych elementów interfejsu, pozwalając użytkownikowi na wykorzystanie jego pełni możliwości. Wymagania niefunkcjonalne mogą w takim razie być skromniejsze względem funkcjonalnych, ale wciąż należy je dokładnie określić, by uniknąć utraty kluczowych elementów przez zaniedbanie tej części.

Wymagania te prezentują się następująco:

\begin{enumerate}
	\item Program powinien wyświetlać obraz z kamery. Jest to najbardziej podstawowe wymaganie, ale także najbardziej istotne. Obserwacja obrazu daje informację zwrotną użytkownikowi, pozwalając na szybką weryfikację działania kamery, a także analizę przystępności otoczenia, w którym dokonuje śledzenia.
	\item Jako że możliwe jest uszkodzenie pliku programu lub brak odpowiedniego sprzętu wymaganego do przeprowadzenia detekcji, istotne jest wdrożenie podstawowej kontroli błędów, zwracającej odpowiednią informację i zakańczającej działanie programu w przypadku wykrycia nieprawidłowości. Informacja ta powinna nakierować użytkownika na źródło problemu, pozwalając na łatwiejsze dojście do jego rozwiązania.
	\item W punkcie \ref{subsec:zakres-dzialania-systemu} wymieniono różne funkcjonalności programu. Część z nich (np. rodzaj wyznaczania punktu odniesienia) z reguły nie będzie działać równolegle, co wymaga prostego sposobu na kontrolowanie, która z funkcjonalności będzie w danym momencie wykorzystywana.
	\item Podczas działania systemu śledzenia ruchu gałek ocznych dokonywana będzie detekcja kilku elementów, takich jak twarz, oczy oraz źrenice. Istotnym aspektem wizualnym jest podkreślenie tych obszarów, dając użytkownikowi możliwość naturalnej kontroli działania rozwiązania na każdym jego etapie. Dodatkowo warto zaznaczyć punkt odniesienia i środek źrenicy, obrazując dystans między nimi. Jest to duża ilość informacji, która może zaciemnić obraz z kamery, dlatego program powinien zadbać o maksymalną przejrzystość.
	\item Poza bardziej zrozumiałym empirycznie obrazowaniem działania systemu śledzącego, takim jak zaznaczanie obszarów detekcji, przydatne może okazać się wizualizowanie technicznego aspektu wykrywania -- prezentując, jak komputer odbiera i przekształca obraz z kamery. Tak przedstawione wyniki, chociaż mniej czytelne dla osób niezaznajomionych, pozwolą użytkownikowi na poprawę działania programu zgodnie z jego własnymi preferencjami.
	\item Przeglądanie zebranych danych może być dużym wyzwaniem, ponieważ komercyjnie dostępne kamery internetowe mogą rejestrować obraz $30$ lub nawet $60$ razy na sekundę, co przy dłuższym śledzeniu generuje ogromną ilość pomiarów. Z tego powodu wymagana jest wizualizacja zebranych danych w prosty sposób, np. za pomocą wykresów czasowych. Takie rozwiązanie pomoże w dalszej analizie i potencjalnej obróbce pomiarów.
\end{enumerate}

\section{Przypadki użycia}
\label{sec:przypadki-uzycia}

Wymagania funkcjonalne i niefunkcjonalne, omawiane w podrozdziałach \ref{sec:wymagania-funkcjonalne} i \ref{sec:wymagania-niefunkcjonalne}, pozwalają dokładnie opisać konkretne elementy systemu, ale mogą utrudniać zrozumienie całości. Diagram UML (ang. \english{Unified Modeling Language}) przypadku użycia pozwala na łatwe zobrazowanie każdego elementu w kontekście całości programu, dopełniając analizę stawianych wymagań. Diagram przypadku użycia prezentowanego systemu widoczny jest na rysunku \ref{fig:UML-use-case}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{pic/use case/use case diagram.png}
	\caption{Diagram UML przypadku użycia systemu do śledzenia ruchu gałek ocznych.}
	\label{fig:UML-use-case}
\end{figure}

Diagram na rysunku \ref{fig:UML-use-case} prezentuje po lewej użytkownika, który pełni rolę aktora wchodzącego w interakcję z systemem wizyjnym do śledzenia ruchu gałek ocznych. Użytkownik może oddziaływać na system na cztery sposoby: rozpoczynając śledzenie, kalibrując punkt odniesienia, personalizując ustawienia oraz wizualizując zebrane pomiary. Akcje są ustawione od góry w kolejności sugerującej korzystanie z programu. Śledzenie ruchu gałek ocznych rozpoczyna się inicjalizacją systemu. Jeśli program nie napotka błędu, rozpoczyna wykrywanie obszarów twarzy. Zawężanie obszarów zainteresowań następuje sekwencyjnie – dopiero gdy pojawi się twarz, można będzie szukać oczu, a następnie źrenic.

Gdy oczy zostaną wykryte, automatycznie utworzony zostanie punkt odniesienia, a gdy detekcja przebiegnie w całości pomyślnie, zapisany zostanie pomiar pozycji źrenicy względem punktu odniesienia oraz zostanie wyświetlony przetworzony obraz pomagający w dostosowaniu czułości. Użytkownik może następnie przeprowadzić kalibrację punktu odniesienia ręcznie lub za pomocą kalibracji zaawansowanej. Może też w każdej chwili wrócić do kalibracji automatycznej.

Personalizacja, pozwalająca dostosować czułość detekcji oraz opcje obrysowania wykrytych rejonów twarzy, może okazać się konieczna w przypadku różnych warunków oświetleniowych lub zmieniających się preferencji użytkownika. Po udanej sesji śledzenia ruchu gałek ocznych użytkownik może zdecydować się na wizualizację zebranych pomiarów, generując wykresy czasowe. W ten sposób omawiany diagram UML przedstawia symbolicznie pełen zakres interakcji użytkownika z systemem oraz kolejność operacji, obrazując działanie aplikacji w typowym scenariuszu.

\section{Narzędzia i metody wykorzystane w pracy}
\label{sec:Narzedzia-i-metody-wykorzystane-w-pracy}

\subsection{ASUS Vivobook 15}
\label{subsec:ASUS-Vivobook-15}

Projekt prezentowany w tej pracy skupia się przede wszystkim na oprogramowaniu, dlatego sprzęt wykorzystany do jego zrealizowania zamyka się w laptopie z wbudowaną kamerką internetową, który jest także źródłem stawianych wymagań w punkcie \ref{subsec:Limitacje-sprzetowe-i-srodowiskowe}. Jest to ASUS Vivobook 15 D1502YA-BQ309, posiadający ośmiordzeniowy procesor AMD Ryzen 7 7730U z zintegrowanym układem graficznym ATI AMD Radeon Graphics. Notebook ten oryginalnie stosowany był do nauki i programowania, co pozwoliło na szybkie przystosowanie go do pracy nad prezentowanym rozwiązaniem. Wpasowuje się on również w założenie limitacji sprzętowych użytkowników.

Wbudowana kamera to USB2.0 HD UVC WebCam producenta Realtek. Urządzenie to rejestruje obraz w 30 klatkach na sekundę w rozdzielczości $1280 \times 720$, posiada także diodę światła białego, która ułatwia śledzenie w niesprzyjających warunkach oświetleniowych i tworzy widoczne na obrazie odbicie rogówkowe. Mimo że nie jest to kamera specjalistyczna, jej parametry są wystarczające do realizacji założeń projektu. Podobne specyfikacje są powszechne w kamerach wbudowanych w laptopy, co sprawia, że opracowane rozwiązanie może być łatwo wdrożone przez użytkowników bez konieczności zakupu dodatkowego sprzętu.

\subsection{Narzędzia programistyczne}
\label{subsec:Narzedzia-programistyczne}

\subsubsection{Github Desktop i Github}

Projekt o takiej skali wymaga użycia systemu kontroli wersji, by zabezpieczyć postępy prac oraz umożliwić bezproblemowe testowanie nowych funkcji. Do przechowywania kodu w chmurze i zabezpieczenia go na zdalnych serwerach wykorzystano hosting GitHub. W celu ułatwienia pracy z systemem kontroli wersji Git użyto aplikacji GitHub Desktop, zapewniającej graficzny interfejs użytkownika, co pozwoliło na wygodniejsze korzystanie z poleceń Git, zatwierdzanie zmian z odpowiednimi wiadomościami, operowanie na gałęziach repozytorium, analizowanie historii zmian oraz synchronizację lokalnego repozytorium z jego zdalnym odpowiednikiem na GitHubie.

\subsubsection{Visual Studio Code (VS Code)}

Visual Studio Code to edytor kodu źródłowego, który świetnie integruje się z GitHub Desktop, a przez to także z GitHubem. VS Code, mimo niskich wymagań systemowych, oferuje wiele funkcjonalności, co pozwoliło na wykorzystanie go jako głównego narzędzia programistycznego. Dzięki kolorowaniu składni, automatycznym podpowiedziom i uzupełnianiu kodu edytor ten przyspiesza pracę i ogranicza liczbę błędów. Obszerna biblioteka wtyczek pozwala na dostosowanie VS Code do własnych potrzeb, np. dodając obsługę języków programowania, takich jak Python czy \LaTeX. Wbudowana integracja z systemem kontroli wersji podkreśla zmodyfikowane pliki, ułatwiając zarządzanie repozytorium. Wbudowana obsługa terminala oraz zaawansowane debugowanie z kontrolą punktów wstrzymania, inspekcją zmiennych i śledzeniem wykonywania kodu w czasie rzeczywistym pozwala na szybką kompilację i naprawę programu, zwłaszcza napisanego w języku Python \cite{bib:VSCode-Essentials}. Podsumowując, Visual Studio Code okazał się niezastąpionym narzędziem w procesie tworzenia zarówno projektu, jak i niniejszej pracy, pozwalając nie tylko na lepszą organizację, ale również na zwiększoną efektywność.

\subsubsection{Python 3.13.0}

Python to wysokopoziomowy język programowania, który charakteryzuje się czytelną i intuicyjną składnią. Cechuje go dynamiczne typowanie, przypisujące typy zmiennych w trakcie działania programu, oraz automatyczne zarządzanie pamięcią. Największym atutem Pythona jest jego popularność i łatwość nauki, co sprawia, że wokół tego języka powstała spora społeczność programistów. Dzięki temu w sieci można znaleźć liczne fora dyskusyjne, obszerne dokumentacje, a także poradniki i kursy wideo, które okazały się niezastąpionym wsparciem w realizacji niniejszej pracy.

Python posiada również obszerną bibliotekę standardową oraz szeroki wybór wysokiej jakości bibliotek zewnętrznych, które oferują gotowe rozwiązania dla niemal każdego zagadnienia programistycznego. Dzięki nim możliwe jest realizowanie złożonych zadań, takich jak analiza danych i przetwarzanie obrazów, często przy użyciu zaledwie kilku linijek kodu \cite{bib:Python-a-byte}. W efekcie Python, zaopatrzony w odpowiednie biblioteki, w przypadku realizacji omawianego rozwiązania, okazał się jedynym językiem programowania potrzebnym do implementacji wszystkich funkcjonalności systemu.

\subsubsection{OpenCV}

OpenCV jest otwartą biblioteką, która posiada ponad $2500$ zoptymalizowanych algorytmów, w tym rozpoznawania obrazu i uczenia maszynowego. Jej dodatkowym atutem jest dokładna dokumentacja oraz szeroka baza użytkowników, aktywnie dzielących się swoimi obserwacjami i wyjaśnieniami na forach dyskusyjnych, co pozwala na sprawne wykorzystanie ogromnego potencjału tej biblioteki \cite{bib:OpenCV-about}. To właśnie dzięki niej możliwe było wykonywanie większości operacji związanych z obrazem w czasie rzeczywistym -- od przechwycenia go przez kamerę, przez wykrycie oraz obrysowanie twarzy i oczu, po binaryzację i operacje morfologiczne.

\subsubsection{NumPy}

NumPy to otwarta biblioteka zapewniająca obsługę dużych, wielowymiarowych i jednolitych macierzy oraz tablic. OpenCV wykorzystuje i przekazuje tablice NumPy w swoich funkcjach, co pozwala na integrację obu bibliotek i łatwe operowanie na obrazach w formie macierzy \cite{bib:NumPy-quick-start}. Popularność tej biblioteki, zwłaszcza w użyciu wraz z OpenCV, sprawia, że jest szeroko omawiana przez społeczność programistyczną, co przekłada się na dużą liczbę poradników i zasobów edukacyjnych \cite{bib:NumPy-OpenCV}. Co za tym idzie, jej zastosowanie w projekcie gwarantowało dostępność wsparcia w rozwiązywaniu potencjalnych problemów.

\subsubsection{Matplotlib}

Matplotlib jest biblioteką do tworzenia wykresów w języku Python, bezproblemowo współpracującą z NumPy. Użycie jej razem z modułem Pyplot pozwala na wizualizację wyników z interfejsem podobnym do programu MATLAB \cite{bib:Matplotlib-pyplot}. Zastosowanie jej w pracy jest uzasadnione dopełnieniem wcześniej wymienionych bibliotek, a także zbieżnością ze znanym już środowiskiem MATLAB.

\subsubsection{pandas}

Pandas to biblioteka do analizy i manipulacji danymi, posiadająca narzędzia do czytania i zapisywania danych w plikach CSV (ang. \english{comma-separated values}) oraz plikach tekstowych \cite{bib:Pandas-about}. Podczas śledzenia ruchu gałek ocznych generowana i zapisywana jest duża ilość danych, a biblioteka ta pozwala na łatwe wydobycie ich z pliku i wykorzystanie w dalszej części programu.

\subsubsection{Time}

Time jest częścią podstawowej biblioteki Pythona i nie wymaga dodatkowej instalacji przed użyciem. Służy do wykonywania operacji związanych z czasem. Zbierane pomiary muszą być zapisywane razem z momentem czasowym, by pozwolić na pełną analizę. Jest to nieskomplikowane zadanie, które można rozwiązać z użyciem tej biblioteki, opierając się na oficjalnej dokumentacji Pythona \cite{bib:time-Python}.

\subsubsection{PyInstaller 6.11.1}

Następnym krokiem, który warto wykonać po zakończeniu prac nad rozwiązaniem, jest ułatwienie jego dystrybucji i użytkowania. W tym celu wykorzystano PyInstaller, który umożliwia spakowanie skryptu Pythona i wszystkich jego zależności do samodzielnego pliku wykonywalnego (exe ang. \english{executable}). Dzięki temu użytkownicy mogą uruchomić program bez konieczności instalowania interpretera Pythona ani dodatkowych modułów. Warto jednak zaznaczyć, że użyto PyInstaller w wersji na system operacyjny Windows, a oprogramowanie to nie pozwala na kompilację krzyżową (czyli wymaga tego samego systemu od użytkownika) \cite{bib:PyInstaller-manual}. Taka forma dystrybucji znacząco ułatwia udostępnianie aplikacji, a także eliminuje problemy związane z niedopatrzeniem w konfigurowaniu środowiska.

\subsection{Metodyka pracy nad projektem}
\label{subsec:Metodyka-pracy-nad-projektem}

Praca nad projektem przebiegała w sposób iteracyjny, co przypomina metodykę zwinnego wytwarzania oprogramowania, a w szczególności podejście stosowane w Scrum, które opiera się na empiryzmie. Proces ten polegał na wyznaczaniu pojedynczych zadań do realizacji, implementowaniu ich rozwiązań, a następnie analizowaniu wyników i dostosowywaniu kolejnych kroków \cite{bib:Scrum-guide}. . Oznacza to, że stawiane wymagania i używane narzędzia, omawiane w rozdziale \ref{ch:wymagania-i-narzedzia}, także były poszerzane w sposób iteracyjny w czasie realizacji projektu. Dzięki takiemu podejściu możliwe było rozpoczęcie pracy z ograniczoną wiedzą i umiejętnościami, a wraz z postępem -- rozbijanie głównego założenia projektu na coraz bardziej szczegółowe elementy, z jednoczesnym nabieraniem doświadczenia w używaniu narzędzi i poszerzaniem wiedzy na temat możliwych rozwiązań. Wraz z postępem funkcjonalności systemu były ulepszane, a problemy zarówno w programie, jak i w założeniach szybko wykrywane i eliminowane.




%--------------------------------------
%
%
% TODO:
%
% Status: 99.9% (basically DONE)
%--------------------------------------
\chapter{Specyfikacja zewnętrzna}
\label{ch:Specyfikacja-zewnetrzna}

\section{Wymagania sprzętowe i programowe}

% w zasadzie trzeba było by to sprawdzić na wirtualce, albo można napisać od czapy że ma wymagania takie jak pyinstaller + kamera, wyświetlacz - NIE! ROBIMY TO NA MENADŻERZE ZADAŃ

By móc korzystać z programu śledzącego ruch gałek ocznych, wymagane jest użycie kamery i komputera wyposażonego w odpowiednie podzespoły oraz oprogramowanie. Jakość i rodzaj wymaganej kamery internetowej są z grubsza bez znaczenia, jako że podczas korzystania z tego systemu można dostosować jego ustawienia, rekompensując niedociągnięcia kamerki internetowej. Równie prostym do wyznaczenia wymogiem jest wolne miejsce na dysku, które odpowiada sumie rozmiarów plików wykonywalnych z teoretycznym zapasem na powiększenie się pliku tekstowego z pomiarami ($10\%$ sumarycznego rozmiaru plików programowych), wynoszącym $100 \ \unit{\mega\byte} \cdot 1.1 = 110 \ \unit{\mega\byte}$. Dzięki plikom wykonywalnym do uruchomienia programu nie jest wymagane także żadne inne oprogramowanie. Aczkolwiek kwestia wymagań dotyczących wydajności procesora, rozmiaru pamięci RAM (ang. \english{random-access memory}) oraz wspieranego systemu operacyjnego wymaga głębszej analizy.

\subsection{Procesor i pamięć RAM}
\label{subsec:Procesor-i-pamięć-RAM}

By ustalić wymagania sprzętowe prezentowanego systemu, przeprowadzono obserwację procentowego użycia procesora oraz pamięci RAM w trakcie detekcji. System okazał się używać średnio $36\%$ procesora, z nagłymi wzrostami nawet do $44,1\%$, W konsekwencji przyjęto, że procentowe użycie tego zasobu nie powinno wzrosnąć ponad $45\%$. Użycie pamięci RAM utrzymywało się na stałym poziomie $170 \ \unit{\mega\byte}$. Jest to na tyle niewielka ilość, że nie stanowi istotnego obciążenia według współczesnych standardów. W związku z tym można przyjąć, że minimalne wymagania pamięciowe pokrywają się z zaleceniami systemu operacyjnego. Znając podzespoły laptopa, omówione w punkcie \ref{subsec:ASUS-Vivobook-15}, można oszacować wymagania sprzętowe minimalne i zalecane.

Do wyznaczenia procesora spełniającego minimalne wymagania stabilnego korzystania z niniejszego rozwiązania posłużono się największą na świecie stroną internetową z testowaniem wzorcowym procesorów, utrzymywaną przez firmę PassMark Software \cite{bib:PassMark-about-us}. Opierając się na punktacji PassMark \cite{bib:PassMark-test}, można oszacować wysokość oceny procesora, który pozwoli na bezproblemowe korzystanie z programu śledzącego. Wiedząc, że procesor laptopa referencyjnego posiada ocenę wysokości $18500$ ($P_{ref}$) \cite{bib:moj-procesor-benchmark}, a program może wykorzystać do $45\%$ jego mocy obliczeniowej ($CPU_{\%ref}$), wyznaczono ilość punktów procesora, który będzie w stanie utrzymać działanie programu przy pełnym obciążeniu jego zasobów ($P_{min}$). Przedstawiono to na równaniu \ref{eq:PassMark-min}.

\begin{align}
	P_{min} &= P_{ref} \cdot CPU_{\%ref} = 18500 \cdot 45\% = 8325
	\label{eq:PassMark-min}
\end{align}

Procesorem, który posiada niewiele wyższą ocenę od obliczonego $P_{min}$, równą $8407$, jest Intel Xeon E5-4650 @ 2.70GHz \cite{bib:min-procesor-benchmark}. Jednakże dla wygody użytkowania oraz zapewnienia płynnej pracy programu w różnych warunkach warto zastosować procesor o wyższej wydajności. Dlatego, przyjmując dodatkowy zapas wydajności na poziomie $20\%$, zalecany wynik PassMark ($P_{rec}$) obliczono za pomocą równania \ref{eq:PassMark-rec}.

\begin{align}
	P_{rec} = 1.2 \cdot P_{min} = 1.2 \cdot 8325 = 9990
	\label{eq:PassMark-rec}
\end{align}

Przystępnym cenowo procesorem posiadającym wynik $10440$, wyższy od obliczonego $P_{rec}$, jest Intel Xeon E5-2660 v2 @ 2.20GHz \cite{bib:ref-procesor-benchmark}. 

\subsection{System operacyjny}
\label{subsec:System-operacyjny}

Kwestia systemu operacyjnego jest przede wszystkim zależna od programu PyInstaller. Jak już wspomniano w punkcie \ref{subsec:Narzedzia-programistyczne}, PyInstaller nie jest narzędziem pozwalającym na utworzenie plików wykonywalnych z kompilacją krzyżową, w związku z tym konieczne jest, by użytkownik korzystał z systemu Windows firmy Microsoft. System wizyjny do śledzenia ruchu gałek ocznych został przetestowany na systemie Windows 11 (również na innej maszynie), gwarantując stabilną pracę w tej konkretnej wersji. Starsze wersje owego systemu operacyjnego, jako że nie zostały przetestowane, nie gwarantują poprawnego funkcjonowania. Warto jednak zaznaczyć, że użyte biblioteki oraz wersja Pythona i PyInstallera wspierają wersje systemu Microsoft od Windows 8 wzwyż, a tak stworzone pliki wykonywalne działają na zasadzie dołączonego interpretatora języka Python \cite{bib:PyInstaller-how-it-works}. W konsekwencji prezentowany w tej pracy program nie powinien napotkać problemu, działając na systemie Windows 8 lub nowszym.

\subsection{Podsumowanie wymagań}
\label{subsec:Podsumowanie-wymagan}

\begin{description}
	\item[System:] Windows 11 (stabilna praca), Windows 8 i Windows 10 (potencjalna kompatybilność -- nie przetestowane)
	\item[Procesor:] Xeon E5-4650 @ 2.70GHz (minimalnie), Intel Xeon E5-2660 v2 @ 2.20GHz lub lepszy (zalecane)
	\item[Pamięć RAM:] $2 \ \unit{\giga\byte}$ (wymaganie systemowe Windows 8 wersja 64-bitowa)
	\item[Miejsce na dysku:] $110 \ \unit{\mega\byte}$
	\item[Dodatkowy sprzęt:] Kamera internetowa (zalecana częstotliwość odświeżania to $30$ klatek na sekundę), klawiatura
\end{description}

\section{Sposób instalacji}
\label{sec:Sposob-instalacji}

Rozwiązanie dzieli się na dwa pliki wykonywalne: \texttt{eye\_\-detect.exe} \allowbreak i \texttt{eye\_\-tracking\_\-plot.exe}, które nie wymagają przeprowadzenia procesu instalacji. By uruchomić owe programy, wystarczy umieścić je na komputerze przy użyciu przenośnej pamięci USB (ang. \english{Universal Serial Bus}) lub pobrać je z repozytorium GitHub albo z udostępnionego folderu na dysku Google pod linkiem: \url{https://drive.google.com/drive/folders/1RGOLKfGqTqCwREWvMt3sQEOPwD7aKOdE?usp=sharing}. 

System do śledzenia ruchu gałek ocznych uruchomi się po otwarciu pliku \texttt{eye\-\_detect\-.exe}, a zebrane dane można wyświetlić w formie wykresów czasowych, otwierając plik \texttt{eye\-\_tracking\-\_plot\-.exe}. Plik \texttt{eye\-\_tracking\-\_data\-.txt} nie jest wymagany do uruchomienia \texttt{eye\-\_detect\-.exe} -- w przypadku jego braku program utworzy go na koniec śledzenia. Jednakże plik ten jest wymagany do uruchomienia \texttt{eye\-\_tracking\-\_plot\-.exe}. 

Plik tekstowy \texttt{eye\_tracking\_data.txt} oraz \texttt{Readme.txt}, zawierający instrukcję instalacji i użytkowania, są dostępne razem z resztą plików wykonywalnych na dysku Google i GitHub.

\section{Kategorie użytkowników}
\label{sec:Kategorie-uzytkownikow}

Prezentowany system wizyjny do śledzenia ruchu gałek ocznych nie jest kierowany do konkretnej grupy odbiorców -- jest on z założenia uniwersalny w zastosowaniu i niewymagający sprzętowo, pozwalając każdej osobie wyposażonej w komputer z kamerą i zainteresowanej okulografią cieszyć się jego możliwościami.

Chociaż, tak jak wspomniano w punkcie \ref{subsec:zakres-dzialania-systemu}, rozwiązanie to oferuje przede wszystkim podstawowe funkcje śledzenia, może ono okazać się użyteczne zarówno dla hobbystów, jak i studentów czy badaczy. Bardziej zaawansowani użytkownicy mogą skorzystać z funkcji pomiaru względem odbicia rogówkowego, na przykład wyposażając system w kamerę NIR (ang. \english{Near-InfraRed}) i diodę podczerwoną. Z drugiej strony, osoby mniej zaznajomione z tematem lub posiadające gorszej jakości sprzęt mogą wykorzystać kalibrację ręczną jako prosty sposób na badanie skupienia uwagi na jednym punkcie ekranu.

\section{Sposób obsługi i działania}
\label{sec:Sposob-obslugi}

W pierwszej kolejności użytkownik musi zdecydować, czy chce rozpocząć śledzenie czy wizualizację pomiarów zebranych w pliku tekstowym, jako że funkcje te uruchamiane są przez osobne pliki wykonywalne. By rozpocząć śledzenie ruchu gałek ocznych, należy otworzyć plik \texttt{eye\_\-detect.exe}, np. poprzez dwukrotne kliknięcie. Otwarty zostanie terminal w lokalizacji, w której znajduje się plik wykonywalny, oraz okno ,,eye detection'', w którym, po uzyskaniu dostępu do kamery, wyświetlony zostanie obraz. Proces uruchamiania pokazano na rysunku \ref{fig:eye_detect-uruchamianie}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/obsługa/okna przy otwarciu.png}
	\caption{Główne okna programu \texttt{eye\_\-detect.exe} w momencie uruchamiania, przed połączeniem z kamerą.}
	\label{fig:eye_detect-uruchamianie}
\end{figure}

Jeśli program uzyska dostęp do kamery, automatycznie zacznie pobierać z niej obraz, który przekształci do odcieni szarości, aby móc wyszukiwać na nim twarz i oczy, używając klasyfikatora Haara. Gdy uda się wyszukać oczy, obraz w odcieniach szarości zostanie zbinaryzowany i poddany operacjom morfologicznym w celu wyszukania źrenic. Wszystkie wykryte elementy zostaną oznaczone na obrazie barwnym i wyświetlone w oknie ,,eye detection''.

Podczas działania programu mogą otworzyć się jeszcze dwa dodatkowe okna: ,,Bin eyes for testing'', pojawiające się w momencie wykrycia oczu, oraz ,,Bin reflection for testing'', które otworzy się w momencie włączenia trybu kalibracji zaawansowanej. ,,Bin eyes for testing'' obrazuje binaryzację źrenic przed (po lewej) i po (po prawej) operacjach morfologicznych, z kolei ,,Bin reflection for testing'' pokazuje binaryzację dla odbicia rogówkowego. Okna te są wizualizacją techniczną, ułatwiającą dostrojenie programu, i są przedstawione na rysunku \ref{fig:eye_detect-okna-bin}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/obsługa/okna binarne.png}
	\caption{Wizualizacja obrazów binarnych wykorzystywanych w procesie wykrywania źrenic i odbicia rogówkowego.}
	\label{fig:eye_detect-okna-bin}
\end{figure}

Użytkownik może wchodzić w interakcję z systemem na dwa sposoby: poprzez interfejs graficzny w formie suwaków widocznych na rysunku \ref{fig:eye_detect-uruchamianie} lub przez naciśnięcie odpowiedniego klawisza na klawiaturze. Nad obrazem w oknie ,,eye detection'' widnieją dwa suwaki, których pozycje można zmieniać poprzez kliknięcie lub przeciąganie myszą.

Suwak ,,Eye thresh'' (pierwszy od góry) odpowiada progowi używanemu w binaryzacji źrenic, a jego wartość ustawiona jest na 15, pozwalając na stabilne śledzenie przy dobrym oświetleniu frontalnym. Jednakże, w zależności od warunków oświetleniowych, a także preferencji związanych ze stabilnością i dokładnością pomiaru, użytkownik powinien ręcznie dostosować wartość progu, aby zapewnić optymalną detekcję źrenic.

Następny suwak ,,Light thresh'' (drugi od góry) używany jest jedynie w przypadku kalibracji zaawansowanej. On także odpowiada progowi binaryzacji, ale w tym przypadku dla odbicia rogówkowego. Użytkownik, wyposażony w odpowiednio intensywne źródło światła i decydujący się na tego typu kalibrację punktu odniesienia, powinien ręcznie wybrać wartość na suwaku tak, aby na obrazie binaryzacji pozostał jedynie niewielki jasny punkt.

Reszta funkcjonalności dostępna jest po naciśnięciu danego klawisza na klawiaturze. Ich wykaz prezentuje się następująco:

\begin{description}
	\item[z (kalibracja automatyczna) --]  tryb kalibracji punktu odniesienia, który ustawiany jest w momencie uruchomienia programu. Pozwala na pomiar ruchu źrenic względem środka obszaru wykrytego oka.
	\item[x (kalibracja zaawansowana) --]  tryb kalibracji punktu odniesienia na podstawie obrazu binarnego oka. Pozwala na pomiar ruchu źrenic względem wykrytego odbicia rogówkowego.
	\item[c (kalibracja ręczna) --]  tryb kalibracji punktu odniesienia na aktualną pozycję źrenic. Pozwala na pomiar ruchu źrenic względem ustalonego miejsca w obszarze wykrytego oka.
	\item[r (przełączanie trybów rysowania) --] dostępne są trzy tryby rysowania wykrytych obszarów na obrazie z kamery:
	\begin{enumerate}
		\item Pierwszy tryb, ustawiony wraz z uruchomieniem programu, obrysowuje twarz, ograniczony obszar twarzy, oczy, źrenice, środki źrenic oraz punkt odniesienia.
		\item Drugi tryb pozwala na większą przejrzystość, rysując jedynie najistotniejsze elementy detekcji, czyli środek źrenic i punkt odniesienia.
		\item Trzeci tryb wyłącza rysowanie, zwracając niezmodyfikowany obraz z kamery.
	\end{enumerate}
	\item[q (wyłączenie programu) --] jedyny poprawny sposób na zatrzymanie systemu, po naciśnięciu bezpiecznie zamyka plik tekstowy z pomiarami, zwalnia kamerę i zamyka wszystkie okna programu.
\end{description}

Po udanej sesji śledzenia ruchu gałek ocznych można zwizualizować zebrane pomiary, zapisane w pliku tekstowym, otwierając plik \texttt{eye\_\-tracking\_\-plot.exe},  umieszczony w tym
samym folderze co plik tekstowy. Wygenerowane zostaną trzy zestawy wykresów w osobnych
oknach, przedstawiające przemieszczenie źrenicy w osi względem punktu odniesienia
(dla obu oczu wspólnie i osobno). Zostały one utworzone przy pomocy Pyplot, pozwalając
użytkownikowi na swobodne przybliżanie i poruszanie się po wykresach. Możliwe
jest także zapisanie utworzonego wykresu w wielu popularnych formatach. Okna z
wykresami wraz z terminalem przedstawione są na rysunku \ref{fig:eye_tracking_plot-wykresy-i-terminal}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/obsługa/wykresy.png}
	\caption{Okna programu \texttt{eye\_\-tracking\_\-plot.exe}, przedstawiające wykresy czasowe przemieszczenia źrenicy i terminal.}
	\label{fig:eye_tracking_plot-wykresy-i-terminal}
\end{figure}

W momencie zamknięcia programu wszystkie okna, w tym okno terminala,
zostają zamknięte. Z tego względu użytkownicy zainteresowani informacjami zwracanymi w
terminalu -- takimi jak komunikaty o błędach, które mogą pojawić się w przypadku
uszkodzonego pliku lub braku dostępu do kamery -- mogą uruchomić dany plik przy użyciu
terminala. Wystarczy ustawić katalog roboczy terminala na ten, w którym znajduje
się plik wykonywalny, a następnie wpisać jego nazwę wraz z rozszerzeniem i nacisnąć Enter.
W ten sposób, po zakończeniu działania programu, informacje w terminalu zostaną
zachowane.

\section{Kwestie bezpieczeństwa}
\label{sec:Kwestie-bezpieczenstwa}

System do śledzenia ruchu gałek ocznych w żaden sposób nie przechowuje ani nie
nagrywa obrazów z kamery komputera, a wszelkie wykorzystywane zasoby zostają zwolnione
w momencie zakończenia działania. Jedyną zapisywaną informacją są pomiary w
pliku tekstowym, dostępnym w tym samym folderze co plik \texttt{eye\_\-detect.exe}, które można
z łatwością usunąć, przenosząc do kosza, a następnie go opróżniając. Prezentowany program
nie korzysta także z połączenia sieciowego, wykonując wszystkie operacje lokalnie,
w katalogu, w którym się znajduje.

PyInstaller ostrzega, że istnieją zagrożenia związane z używaniem plików wykonywalnych
w formie jednego pliku. PyInstaller rozpakowuje część swoich zasobów do
tymczasowego folderu (nazwy zaczynające się od \_MEI) podczas uruchamiania aplikacji.
Jeśli program zostanie nieoczekiwanie przerwany, np. przez awarię lub wymuszone
zamknięcie przez Menedżera Zadań, folder ten może nie zostać usunięty. W skrajnych przypadkach,
przy częstych awariach, może dojść do gromadzenia się niepotrzebnych plików, co w
długoterminowej perspektywie może zużywać miejsce na dysku. Możliwe jest ręczne
usunięcie nadmiarowych plików, które w przypadku systemu Windows zazwyczaj znajdują
się w folderze Temp -- jego lokalizację można wyszukać, wpisując komendę \texttt{ECHO \-\%Temp\%} w Wierszu poleceń. 

Nie należy także otwierać plików wykonywalnych utworzonych przez
PyInstaller z uprawnieniami administratora, ponieważ daje to potencjalną możliwość
modyfikowania plików w tymczasowym folderze przez atakującego. Taka modyfikacja mogłaby
prowadzić do eskalacji uprawnień \cite{bib:PyInstaller-how-it-works}.

\section{Scenariusze korzystania z systemu}
\label{Scenariusze-korzystania-z-systemu}

Dla lepszego zobrazowania możliwości systemu śledzącego ruch gałek ocznych przedstawione
zostaną kroki przykładowego scenariusza, ilustrowane zrzutami ekranu.

\begin{enumerate}
	\item Użytkownik decyduje się na podjęcie śledzenia, więc uruchamia plik wykonywalny \texttt{eye\_\-detect.exe} (rysunek \ref{fig:scenariusz-odpalanie-sledzenia}).

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/odpalamy śledzenie.png}
		\caption{Użytkownik decyduje się na rozpoczęcie śledzenia.}
		\label{fig:scenariusz-odpalanie-sledzenia}
	\end{figure}

	\item Program nie napotkał żadnych błędów i uruchomił się poprawnie, wyświetlając obraz
	z kamery. Źrenice nie zostały wykryte z powodu niesprzyjającego oświetlenia (rysunek \ref{fig:scenariusz-siema}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/śledzenie start.png}
		\caption{Użytkownik uruchomił pomyślnie system do śledzenia ruchu gałek ocznych.}
		\label{fig:scenariusz-siema}
	\end{figure}

	\item Użytkownik poprawił swoje otoczenie, oświetlając twarz od frontu i odsłaniając
	włosy, tak aby nie przeszkadzały w detekcji. Zmieniono także ustawienia, zwiększając
	próg detekcji i kalibrując punkt odniesienia poprzez spojrzenie na środek ekranu
	oraz naciśnięcie klawisza ,,c'' (rysunek \ref{fig:scenariusz-poprawione-swiatlo}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/poprawienie śledzenia.png}
		\caption{Użytkownik poprawił warunki oświetleniowe i dostosował ustawienia programu.}
		\label{fig:scenariusz-poprawione-swiatlo}
	\end{figure}

	\item Celem śledzenia była analiza ruchu gałek ocznych w trakcie oglądania krótkiego
	filmu na platformie YouTube (rysunek \ref{fig:scenariusz-yt}) \cite{bib:youtube}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/filmik yt.png}
		\caption{Użytkownik uruchamia krótkie nagranie do analizy uwagi wzrokowej.}
		\label{fig:scenariusz-yt}
	\end{figure}

	\item Po przeprowadzonym eksperymencie naciśnięto klawisz ,,q'', zakańczając pomiar.
	Użytkownik następnie zdecydował się na wygenerowanie wykresów czasowych do
	analizy i uruchomił plik \texttt{eye\_\-tracking\_\-plot.exe} (rysunek \ref{fig:scenariusz-odpalanie-wykresy}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/odpalam wykresy.png}
		\caption{Użytkownik decyduje się na wygenerowanie wykresów czasowych z zebranych pomiarów.}
		\label{fig:scenariusz-odpalanie-wykresy}
	\end{figure}

	\item Analiza zebranych pomiarów w tym przypadku najlepiej widoczna jest na wykresach
	obu oczu jednocześnie. Filmik trwał $33$ sekundy, a spojrzenie kierowane było głównie
	na środek ekranu i wzdłuż osi $Y$, co jest widoczne na wykresie (rysunek \ref{fig:scenariusz-analiza-wykresy}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/analiza wykresów.png}
		\caption{Użytkownik analizuje wykresy dla obu oczu.}
		\label{fig:scenariusz-analiza-wykresy}
	\end{figure}

	\item Użytkownik przybliżył wykresy do interesującego fragmentu, a następnie zmodyfikował
	rozmieszczenie i wielkość pól wykresów dla lepszej widoczności (rysunek \ref{fig:scenariusz-ustawianie-wykresy}).
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/dostosowanie wykresów.png}
		\caption{Użytkownik dostosowuje ustawienia wizualne wykresów.}
		\label{fig:scenariusz-ustawianie-wykresy}
	\end{figure}

	\item Na koniec wykresy zostały zapisane. Użytkownik może wybrać format, który najbardziej
	odpowiada jego wymaganiom i preferencjom (rysunek \ref{fig:scenariusz-zapisywanie-wykresy})
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{pic/scenariusz/zapisanie wykresow.png}
		\caption{Użytkownik zapisuje wykresy czasowe w wybranym przez siebie rozszerzeniu.}
		\label{fig:scenariusz-zapisywanie-wykresy}
	\end{figure}
\end{enumerate}





%--------------------------------------
%
%
% TODO:  
%
% Status: 99% (basically DONE)
%--------------------------------------
\chapter{Specyfikacja wewnętrzna}
\label{ch:Specyfikacja-wewnetrzna}

\section{Przedstawienie idei}
\label{sec:Przedstawienie-idei}

Z powodu architektury programu \texttt{eye\-\_detect.exe} zrozumienie jego idei, opisanej jedynie słownie, może okazać się wyzwaniem. Dlatego stworzono diagram aktywności, który obrazuje wszystkie możliwe ścieżki działania programu w jednolity sposób. Niestety, nawet taka forma opisu, chociaż bardziej przejrzysta, wciąż jest bardzo obszerna. W związku z tym diagram podzielony został na cztery osobne rysunki (rysunki \ref{fig:diagram-aktywnosci-1}, \ref{fig:diagram-aktywnosci-2}, \ref{fig:diagram-aktywnosci-3} i \ref{fig:diagram-aktywnosci-4}), które należy rozumieć jako jedną całość. Rozwiązanie to można podzielić na kilka części.

Inicjalizacja to część programu, wykonująca się tylko raz na jego początku, tworząc podstawowy interfejs oraz zmienne, w tym do przechowywania informacji dla obu oczu. Otwierany jest także plik tekstowy do zapisu pomiarów, w którym zapisywany jest nagłówek. W tej części znajduje się również kontrola błędów, sprawdzająca poprawność załadowania klasyfikatorów Haara i połączenia z kamerą.

Następnie program wchodzi w główną, nieskończoną pętlę. To w niej wykonywane są wszystkie funkcje systemu. W pętli tej pobierana jest klatka z kamery, na której następnie wykrywane są wszystkie możliwe twarze i oczy kaskadą Haara. W zależności od sposobu kalibracji wyznaczany jest także punkt odniesienia. Następnie wykrywane są źrenice poprzez binaryzację i operacje morfologiczne obrazu w odcieniach szarości.

Gdy wyznaczenie punktu odniesienia i środka źrenicy się powiodło, program oblicza przesunięcie względem nich w osiach $X$ i $Y$, a następnie zapisuje pomiar z indeksem oka i momentem czasowym do pliku tekstowego. W przypadku niepowodzenia na jednym z etapów detekcji system po prostu przechodzi dalej, czekając na zmianę.

W trakcie wykonywania się pętli program sprawdza spełnienie warunków, by ustalić sposób kalibracji, a także tryb rysowania, determinujący sposób oznaczania wykrytych elementów. Na koniec wyświetlany jest obraz z kamery po wszystkich modyfikacjach, a także sprawdzane jest wprowadzenie przycisku z klawiatury, które może zmienić wartości zmiennych, determinując sposób rysowania i kalibracji. Jeśli użytkownik naciśnie klawisz ,,q'', pętla zostaje przerwana, zasób kamery zostaje zwolniony, plik tekstowy zostaje bezpiecznie zamknięty i wszystkie okna zostają zniszczone, zakańczając działanie \texttt{eye\-\_detect.exe}. 

Program \texttt{eye\_\-tracking\_\-plot.exe}, ze względu na swoją nieskomplikowaną strukturę, nie posiada diagramu aktywności. Najpierw odczytywany jest plik \texttt{eye\_tracking\_data.txt} z oznaczeniem separatora danych. Następnie wykreślane są trzy wykresy złożone -- za każdym razem w analogiczny sposób, z użyciem funkcji biblioteki Matplotlib.pyplot: ustalany jest rozmiar wykresów i współdzielenie osi X; program przechodzi przez wszystkie dane w pliku, grupuje indeksami i ustala wartości na osiach; na wykres nanoszone są punkty, które łączone są prostą linią o odpowiednim kolorze; ustalane zostają ustawienia wykresów, takie jak nazwy osi, tytuły i obecność siatki.

Program przeprowadza te operacje dla każdego wykresu złożonego, który prezentowany jest w osobnych oknach. Dzięki temu wyświetlone zostają wykresy przemieszczenia w obu osiach -- dla każdego oka z osobna oraz wspólnie.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth, height=0.96\textheight]{pic/diagram aktywności/druga próba/diagram aktywności 1.png}
	\caption{Diagram aktywności programu \texttt{eye\-\_detect.exe} (część 1).}
	\label{fig:diagram-aktywnosci-1}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth, height=0.96\textheight]{pic/diagram aktywności/druga próba/diagram aktywności 2.png}
	\caption{Diagram aktywności programu \texttt{eye\-\_detect.exe} (część 2).}
	\label{fig:diagram-aktywnosci-2}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/diagram aktywności/druga próba/diagram aktywności 3.png}
	\caption{Diagram aktywności programu \texttt{eye\-\_detect.exe} (część 3).}
	\label{fig:diagram-aktywnosci-3}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/diagram aktywności/druga próba/diagram aktywności 4.png}
	\caption{Diagram aktywności programu \texttt{eye\-\_detect.exe} (część 4).}
	\label{fig:diagram-aktywnosci-4}
\end{figure}
\FloatBarrier

\section{Architektura systemu}
\label{sec:Architektura-systemu}

System wizyjny do śledzenia ruchu gałek ocznych opiera się na prostej architekturze. Oba programy posiadają strukturę monolityczną. Struktura ta składa się z jednej warstwy, w której realizowane są wszystkie funkcje programu. W przypadku \texttt{eye\_\-tracking\_\-plot.exe} wykonywane są one tylko raz, natomiast \texttt{eye\-\_detect.exe} wykonuje większość operacji w głównej pętli programu do momentu jej przerwania. Kod można opisać jako napisany w bardzo podstawowym paradygmacie imperatywnym, czyli takim, który operuje na ciągu instrukcji zmieniających stan programu, nie dzieląc go na własne obiekty, klasy czy struktury funkcji. Program zmienia swój stan poprzez sekwencyjne instrukcje, korzystając z pętli i warunków do określenia następnej ścieżki wykonywania.

Kolejnym aspektem tego rozwiązania jest manipulacja zasobami komputera w sposób bezpośredni -- zarówno względem pamięci komputera, używając zmiennych globalnych, jak i zasobów sprzętowych, łącząc się z kamerą i pobierając znaki ASCII (ang. \english{American Standard Code for Information Interchange}) z klawiatury. Nie dochodzi także do bezpośredniej komunikacji pomiędzy \texttt{eye\-\_detect.exe} a \texttt{eye\_\-tracking\_\-plot.exe}. Programy te są samodzielne i niezależne -- ich jedynym elementem wspólnym jest wykorzystywanie pliku tekstowego \texttt{eye\_tracking\_data.txt}, za pośrednictwem którego dochodzi do przekazywania informacji.

\section{Opis struktur danych}
\label{sec:Opis-struktury-danych}

By móc omówić zapisywane dane, należy najpierw opisać, w jaki sposób system przechowuje wartości w zmiennych. Program \texttt{eye\-\_detect\-.exe} wykorzystuje zarówno zmienne będące instancjami klas bibliotek, jak i zmienne pojedyncze, przechowujące wartości progów binaryzacji, czasu oraz stanów wykorzystywanych w warunkach logicznych. Większość zmiennych używanych w kodzie to jednak listy dwuelementowe, w których wartości są uporządkowane względem pozycji dla obu oczu. Z kolei program \texttt{eye\_\-tracking\_\-plot\-.exe} korzysta wyłącznie z instancji klas, przechowujących wartości pomiarów wczytywanych z pliku tekstowego.

Po zakończeniu działania \texttt{eye\-\_detect\-.exe} oraz \texttt{eye\_\-tracking\_\-plot\-.exe} wszystkie zmienne zostają zwolnione. Przy ponownym uruchomieniu tych programów zmienne są tworzone na nowo, z wartościami ustalonymi w procesie inicjalizacji. W związku z tym prezentowany system nie zapisuje swoich ustawień. Jedyną formą zapisu danych pomiarowych jest ich zapis w pliku tekstowym przez program \texttt{eye\-\_detect.exe}. Dane w pliku \texttt{eye\-\_tracking\-\_data.txt} są przechowywane w następujący sposób:

\begin{enumerate}
	\item Zapisanie pierwszej linijki w formie nagłówka za pomocą \lstinline|file.write('Eye_Index time X_Displacement Y_Displacement\n')|. Funkcja ta wstawia tekst. Nazwy kolumn oddzielane są spacjami, a zakończone znakiem końca linii.
	\item Wstawianie wyliczonych wartości za pomocą \lstinline|file.write(f'{i} {program_runtime:.2f} {displacement_x:.2f} {displacement_y:.2f}\n')| w pętli. Zmienna \texttt{i} oznacza indeks oka i jest wyznaczona przez sortowanie wykrytych oczu. \texttt{program\-\_runtime} to moment czasowy pomiaru, obliczany przez odjęcie momentu uruchomienia programu od aktualnego czasu, a zmienne \texttt{displacement\-\_x} i \texttt{displacement\-\_y} to przesunięcie źrenicy względem punktu odniesienia w odpowiadających osiach. Tutaj także dane oddzielone są spacjami, a zakończone znakiem końca linii.
\end{enumerate}

Czas wyrażony jest w sekundach, a przesunięcie w pikselach. Obie wartości zapisywane są z dokładnością do dwóch miejsc po przecinku.

\section{Funkcjonalności bibliotek}
\label{sec:Funkcjonalnosci-bibliotek}

Prezentowany system dzieli się na dwa oddzielne moduły \texttt{eye\-\_detect\-.exe} i \texttt{eye\_\-tracking\_\-plot\-.exe}, które korzystają z odrębnych bibliotek i ich funkcjonalności. Omówione zostaną istotniejsze z nich.

\subsection{\texttt{eye\-\_detect\-.exe}}
\label{subsec:eye-detect.exe}

\subsubsection{OpenCV (cv2)}

\begin{description} 
	\item[\texttt{cv2.VideoCapture.read():}] Pobiera pojedynczą klatkę z kamery i zwraca wynik powodzenia jako wartość \texttt{bool} oraz klatkę jako obraz.
	\item[\texttt{cv2.CascadeClassifier.detectMultiScale():}] Wykrywa obiekty (np. twarze) na obrazie, zwracając listę prostokątów wokół wykrytych elementów.
	\item[\texttt{cv2.cvtColor():}] Konwertuje obraz między różnymi przestrzeniami kolorów, np. z BGR na skalę szarości.
	\item[\texttt{cv2.threshold():}] Przekształca obraz na obraz binarny, ustawiając piksele z wartością poniżej lub powyżej danej wartości progu na $0$ lub $255$.
	\item[\texttt{cv2.morphologyEx():}] Stosuje operacje morfologiczne (np. otwarcie, zamknięcie) do przetwarzania obrazu.
	\item[\texttt{cv2.findContours():}] Wyszukuje kontury obiektów w obrazie binarnym i zwraca ich współrzędne.
	\item[\texttt{cv2.minEnclosingCircle():}] Znajduje najmniejszy okrąg obejmujący dany zestaw punktów (np. kontur obiektu).
	\item[\texttt{cv2.imshow():}] Wyświetla obraz w osobnym oknie.
	\item[\texttt{cv2.createTrackbar():}] Tworzy suwak w interfejsie OpenCV, który może służyć np. do dynamicznej zmiany parametrów.
	\item[\texttt{cv2.rectangle():}] Rysuje prostokąt na obrazie, podając współrzędne jego rogów, kolor i grubość linii. W programie użyte zostały także analogiczne funkcje rysujące linie i okręgi.
	\item[\texttt{cv2.waitKey():}] Oczekuje na naciśnięcie klawisza przez określony czas (w milisekundach) i zwraca jego kod.
\end{description}

Każda z tych funkcji jest dokładnie opisana w dokumentacji OpenCV \cite{bib:OpenCV-funkcje}.

\subsubsection{numpy (np)}

\begin{description}
	\item[\texttt{np.concatenate():}] Łączy (konkatenacja) dwie lub więcej tablic NumPy wzdłuż określonej osi (np. poziomo lub pionowo).
	\item[\texttt{np.ones():}] Tworzy tablicę NumPy wypełnioną jedynkami o podanym rozmiarze i opcjonalnym typie danych.
\end{description}

Każda z tych funkcji jest dokładnie opisana w dokumentacji numpy \cite{bib:numpy-funkcje}.

\subsubsection{time}

\begin{description}
	\item[\texttt{time.time()}] Zwraca aktualny czas w sekundach jako liczbę zmiennoprzecinkową, mierzoną od ustalonej epoki. Jest często używana do mierzenia czasu wykonania kodu.
\end{description}

Funkcja \texttt{time.time()} jest dokładniej opisana w dokumentacji modułu time \cite{bib:time-Python}.

\subsection{\texttt{eye\_\-tracking\_\-plot\-.exe}}
\label{subsec:eye-tracking-plot.exe}

\subsubsection{Matplotlib.pyplot (plt)}

\begin{description}
	\item[\texttt{plt.subplots()}] Tworzy nową figurę Matplotlib i zestaw osi, zwracając obiekt figury oraz tablicę obiektów osi. Umożliwia łatwe rysowanie wielu wykresów w jednym oknie.
	\item[\texttt{plt.tight\_layout()}] Automatycznie dostosowuje odstępy między wykresami, aby uniknąć nachodzenia na siebie elementów, poprawiając czytelność.
	\item[\texttt{plt.show()}] Wyświetla aktualną figurę Matplotlib na ekranie, zatrzymując wykonanie programu do momentu zamknięcia okna wykresu (w trybie interaktywnym).
	\item[\texttt{matplotlib.axes.Axes.plot()}] Rysuje wykres na określonej osi, przyjmując jako argumenty dane (np. wartości $x$ i $y$) oraz opcjonalne style linii, kolory i etykiety.
	\item[\texttt{plt.axes.Axes.set\_ylabel()}] Ustawia etykietę osi Y, określając jej nazwę (np. jednostkę miary lub opis).
	\item[\texttt{plt.axes.Axes.set\_title()}] Nadaje tytuł wykresowi na danej osi, pomagając opisać prezentowane dane.
	\item[\texttt{plt.axes.Axes.legend()}] Dodaje legendę do wykresu, opartą na etykietach linii/serii danych, co ułatwia ich rozróżnienie.
	\item[\texttt{plt.axes.Axes.grid()}] Włącza lub wyłącza siatkę na wykresie.
\end{description}

Każda z tych funkcji jest dokładnie opisana w dokumentacji Matplotlib.pyplot \cite{bib:matplotlib-pyplot-funkcje}.

\subsubsection{pandas (pd)}

\begin{description}
	\item[\texttt{pd.read\_csv()}] Wczytuje dane z pliku CSV (lub podobnego, np. TXT ang. \english{text file}) do obiektu DataFrame, umożliwiając określenie separatora, nazw kolumn, pominiętych wierszy itp.
	\item[\texttt{pd.DataFrame.groupby()}] Grupuje dane w \texttt{DataFrame} według jednej lub więcej kolumn, zwracając obiekt grupujący.
\end{description}

Każda z tych funkcji jest dokładnie opisana w dokumentacji pandas \cite{bib:pandas-funkcje}.

\section{Przegląd algorytmów}
\label{sec:Przegląd-algorytmow}

\subsection{Kaskadowy klasyfikator oparty na cechach Haara}
\label{subsec:kaskadowy-klasyfikator-Haara}

Jest to najważniejszy algorytm w kontekście prezentowanego rozwiązania. Dzięki niemu możliwe jest wykrywanie twarzy i oczu, bez których detekcja źrenic byłaby znacznie bardziej skomplikowana (jeśli w ogóle możliwa przy zastosowaniu sprzętu o jakości założonej w punkcie \ref{subsec:Limitacje-sprzetowe-i-srodowiskowe}). Metoda ta została zaproponowana przez Paula Violę i Michaela Jonesa w artykule ,,\english{Rapid Object Detection using a Boosted Cascade of Simple Features}'' \cite{bib:Rapid-object-detection}. 

Zasada działania opiera się na uczeniu maszynowym, w którym funkcję kaskadową trenuje się na dużej liczbie obrazów pozytywnych (zawierających szukany obiekt) i negatywnych (niezawierających szukanego obiektu). Gdy klasyfikator zostanie wytrenowany, testuje się jego działanie, podając inne obrazy niż te, na których był uczony. W tym przypadku szukanymi obiektami są twarze oraz oczy. Wykorzystane klasyfikatory są już wyuczone i dostępne w bibliotece OpenCV jako pliki XML (ang. \english{Extensible Markup Language}) (\texttt{haarcascade\_\-frontalface\_\-default\-.xml}, \texttt{haarcascade\_\-eye.\-xml}).

Do wyodrębnienia cech szukanego obiektu używane są tak zwane cechy Haara. Cechy te to pojedyncza wartość obliczana jako różnica sumy pikseli znajdujących się w białym obszarze od sumy pikseli znajdujących się w obszarze czarnym danej maski. Przykładowe maski splotowe Haara pokazane są na rysunku \ref{fig:Cechy-Haara}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pic/cechy haara/cechy Haara.png}
	\caption{Jądra splotowe Haara pozwalające wykrycie między innymi krawędzi i lini.}
	\label{fig:Cechy-Haara}
\end{figure}

Najpierw do ustalenia cech używa się wszystkich masek splotowych o każdym możliwym rozmiarze i położeniu na wszystkich obrazach testowych, za każdym razem licząc sumę pikseli. Aby usprawnić sumowanie danego obszaru pikseli, używa się obrazu całkowego (sum skumulowanych), w którym każdy piksel jest sumą oryginalnego piksela oraz piksela nad nim i na lewo w obrazie całkowym. Dzięki obrazowi sum skumulowanych można obliczyć sumę pikseli dowolnego prostokątnego obszaru na obrazie, używając jedynie czterech wartości pikseli na jego rogach. Można to przedstawić wzorem \ref{eq:obraz-calkowy}, gdzie $i(x,y)$ oznacza wartość piksela w oryginalnym obrazie na współrzędnych $(x,y)$, a $I(x,y)$ -- wartość w obrazie całkowym.

\begin{align}
	\sum_{\substack{x_0 < x \leq x_1 \\ y_0 < x \leq y_1}} i(x,y) &= I(x_1,y_1) + I(x_0,y_0) - I(x_1,y_0) - I(x_0,y_1)
	\label{eq:obraz-calkowy}
\end{align}

Większość wyznaczonych cech jest nieistotna, dlatego należy wybrać te, które opisują szukany obiekt. Do tego zadania używa się algorytmu uczenia maszynowego AdaBoost. Na początku wszystkie obrazy posiadają taką samą wagę (wszystkie są traktowane jednakowo), następnie przeprowadza się testowanie każdej cechy na każdym obrazie i wyznacza taki próg, który najlepiej rozdziela obrazy pozytywne i negatywne. Wagi obrazów, które zostały źle sklasyfikowane, zostają zwiększone, a następnie proces testowania i wyznaczania progu jest powtarzany, po czym ponownie modyfikuje się wagi problematycznych obrazów. Algorytm wykonuje się do momentu spełnienia kryterium stopu (np. osiągnięcia zadanej dokładności). Ostateczny klasyfikator składa się ze zbioru wielu słabych klasyfikatorów, które same w sobie nie są w stanie klasyfikować obrazów, ale wspólnie już tak. W ten sposób ograniczona zostaje liczba cech, które pozwalają na wykrycie obiektu na obrazie.

Ostatnim elementem omawianego algorytmu jest jego kaskadowość. W przetwarzanych obrazach większość przestrzeni nie jest obiektem poszukiwań, zatem analizowanie każdego fragmentu przy użyciu wszystkich wyodrębnionych cech byłoby nieefektywne i czasochłonne. Rozwiązaniem są kaskady klasyfikatorów, polegające na podzieleniu wszystkich cech na etapy, które używane są pojedynczo w określonej kolejności. Jeśli analizowany fragment przejdzie etap, zostaje przepuszczony do następnego, a jeśli nie -- zostaje odrzucony i można przesunąć okno algorytmu do następnego fragmentu obrazu. Fragment zostaje uznany za szukany obiekt, jeśli przejdzie przez wszystkie etapy.

Rozłożenie cech na etapy jest nieregularne -- pierwsze etapy zazwyczaj posiadają niewielką liczbę najbardziej istotnych cech, które pozwalają na szybkie odrzucenie większości obszarów niebędących poszukiwanym obiektem. Kolejne etapy zawierają coraz bardziej szczegółowe cechy, które zwiększają pewność detekcji. Dzięki temu obszary obrazu, które ewidentnie nie zawierają obiektu, są eliminowane na wczesnych etapach przetwarzania, co znacząco przyspiesza działanie algorytmu i redukuje liczbę niepotrzebnych obliczeń \cite{bib:Haar-OpenCV}.

\subsection{Binaryzacja i operacje morfologiczne}
\label{subsec:Binaryzacja-i-operacje-morfologiczne}

Metoda wykorzystana do detekcji rejonu twarzy i oczu mogłaby również zostać użyta do wykrycia źrenic. Jednakże zastosowanie binaryzacji i operacji morfologicznych stanowi znacznie prostszą i równie skuteczną alternatywę, eliminującą konieczność stosowania złożonych technik uczenia maszynowego.

Binaryzacja obrazu przeprowadzana jest przez proste progowanie. Jeśli dany piksel ma wartość mniejszą lub równą ustalonemu progowi, to przyjmuje wartość zero, w przeciwnym przypadku ustawiana jest maksymalna wartość. W prezentowanym rozwiązaniu wykorzystano dwa rodzaje binaryzacji.

Pierwszy typ binaryzacji to klasyczne progowanie, stosowane bezpośrednio na niezmodyfikowanym obrazie monochromatycznym (oznaczone jako THRESH\_BINARY w OpenCV). Można je opisać wzorem \ref{eq:binaryzacja-prosta}. Metoda ta pozwala na wykrycie jasnych obiektów, dlatego użyta została w detekcji odbicia rogówkowego.

\begin{align}
    \texttt{dst}(x,y) =
    \begin{cases} 
        \texttt{maxval}, & \text{gdy } \texttt{src}(x,y) > \texttt{thresh} \\
        0, & \text{w przeciwnym wypadku}
    \end{cases}
	\label{eq:binaryzacja-prosta}
\end{align}

Drugim typem jest progowanie w negatywie, które działa na tej samej zasadzie, ale na obrazie o odwróconych wartościach pikseli (oznaczone jako THRESH\_BINARY\_INV w OpenCV). Można je przedstawić wzorem \ref{eq:binaryzacja-negatyw}. Dzięki temu możliwe jest wykrywanie ciemnych obiektów, co zostało wykorzystane do identyfikacji źrenic \cite{bib:Binaryzacja-OpenCV}.

\begin{align}
    \texttt{dst}(x,y) =
    \begin{cases} 
        0, & \text{gdy } \texttt{src}(x,y) > \texttt{thresh} \\
        \texttt{maxval}, & \text{w przeciwnym wypadku}
    \end{cases}
	\label{eq:binaryzacja-negatyw}
\end{align}

Operacje morfologiczne to proste przekształcenia wykonywane na podstawie kształtu danego obiektu, w tym przypadku na obrazie binarnym. Polegają na przemieszczaniu elementu strukturalnego -- okna o dowolnym kształcie i wielkości, z wartościami $0$ oraz $1$ -- po obrazie i porównywaniu ich zgodności.

Pierwszą podstawową operacją morfologiczną jest erozja. W jej przypadku okno przemieszcza się po oryginalnym obrazie, analizując piksel centralny. Jeśli wszystkie piksele znajdujące się pod elementem strukturalnym są równe $1$, wartość punktu centralnego również wynosi $1$. W przeciwnym razie dochodzi do erozji, czyli piksel przyjmuje wartość $0$. W efekcie piksele znajdujące się blisko krawędzi zostają usunięte, co powoduje zmniejszenie obiektu.

Drugą podstawową operacją morfologiczną jest dylatacja, będąca odwrotnością erozji. Jeśli chociaż jeden piksel znajdujący się pod elementem strukturalnym ma wartość $1$, punkt centralny również przyjmuje wartość $1$. Proces ten powoduje powiększenie obiektów, tworząc dodatkową warstwę na ich krawędziach \cite{bib:Morfologia-OpenCV}.

Do wykrycia źrenic zastosowano kombinację erozji i dylatacji w celu usunięcia zakłóceń oraz ujednolicenia kształtu. Najpierw przeprowadzono otwarcie, czyli erozję, po której następuje dylatacja. Dzięki temu usunięto niewielkie obiekty wykryte w tle. Następnie zastosowano domknięcie, czyli dylatację, po której następuje erozja, co pozwoliło wyeliminować niewielkie dziury wewnątrz wykrytej źrenicy.


\FloatBarrier

\section{Szczegóły implementacji wybranych fragmentów}
\label{sec:szczegoly-implementacji-wybranych-fragmentow}

\subsection{Kontrola błędów i zasoby}
\label{subsec:Kontrola-bledow}

Na rysunku \ref{fig:kod-kontrola-bledow} widoczne jest wdrożenie podstawowej kontroli błędów programu i uzyskanie dostępu do potrzebnych zasobów. Pierwszym krokiem w implementacji jest załadowanie gotowych klasyfikatorów Haara, które służą do detekcji twarzy i oczu na obrazie. OpenCV udostępnia wbudowane modele klasyfikatorów w katalogu \texttt{cv2\-.data\-.haarcascades}. Jeśli pliki klasyfikatorów nie zostaną znalezione lub ich wczytanie się nie powiedzie, zwracana jest wartość pusta, co skutkuje błędem. Kolejnym krokiem jest uruchomienie kamery w celu przechwycenia obrazu w czasie rzeczywistym. Podobnie jak w przypadku klasyfikatorów, sprawdzana jest poprawność otwarcia kamery.

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		# Ładowanie klasyfikatorów Haar (gotowe modele służące do wykrywania twarzy i oczu na obrazie)
		face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Wykrywa twarz
		eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml') # Wykrywa oczy
		
		# Sprawdzenie, czy klasyfikatory zostały poprawnie załadowane
		if face_cascade.empty() or eye_cascade.empty():
			raise IOError("Nie można załadować klasyfikatorów Haar")
		
		
		# Uruchomienie kamerki
		cap = cv2.VideoCapture(0) # 0 to pierwsza kamera
		
		# Sprawdzenie, czy kamera została poprawnie otwarta
		if not cap.isOpened():
			raise IOError("Nie można otworzyć kamery")
	\end{lstlisting}
\caption{Kod łączący z zasobami i kontrolą błędu systemu.}
\label{fig:kod-kontrola-bledow}
\end{figure}

\subsection{Wykrywanie twarzy i wyznaczcie ROI}
\label{subsec:Wykrywanie-twarzy-i-wyznaczcie-ROI}

Fragment kodu na rysunku \ref{fig:kod-wykrywanie-twarzy} odpowiada za wykrywanie twarzy na obrazie z kamery oraz przygotowanie odpowiedniego obszaru do analizy oczu. Do wykrycia twarzy użyto metodę wykorzystującą wcześniej wczytaną kaskadę Haara, operującą na obrazie monochromatycznym podanym w pierwszym parametrze. Drugi parametr determinuje, o ile zmniejszany jest rozmiar obrazu przy każdej skali. Dzięki temu większe twarze wciąż mogą być wykrywane przez algorytm. Wartość $1.3$ zapewnia szybkie wykrywanie kosztem większej szansy pominięcia twarzy o danym rozmiarze. Trzeci parametr determinuje liczbę sąsiadów potrzebną do poprawnej detekcji twarzy. Podczas detekcji pojawia się dużo pojedynczych fałszywych pozytywów, dlatego ustawienie wartości 5 oznacza, że za twarz można uznać jedynie ten obszar, który został wykryty pięciokrotnie lub więcej, odrzucając fałszywe pozytywy.

Metoda zwraca wielkość i lokalizację prostokąta, w którym znajduje się twarz względem klatki z kamery, a program przechodzi w pętli po każdej wykrytej twarzy. Jako że otwory nosowe sprawiają problemy w detekcji oczu, wycięty zostaje region zainteresowania obejmujący $\frac{2}{3}$ twarzy, licząc od czoła, i to na nim przeprowadzono dalsze wykrywanie. Widoczna jest też funkcja rysująca prostokąt wykrytej twarzy -- jeśli tryb rysowania jest włączony. W analogiczny sposób oznaczane są inne rejony twarzy.

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		# Wykrywanie twarzy
        faces = face_cascade.detectMultiScale(gray, 1.3, 5) # skalowanie obrazu 1.3, liczba trafień żeby uznać twarz 5 # faces to prostokąty

        
        # Dla każdej twarzy
        for (x, y, w, h) in faces: # współrzędna X górnego lewego rogu prostokąta # współrzędna Y górnego lewego rogu prostokąta # szerokość prostokąta # wysokość prostokąta # całość dla każdej twarzy

            if draw_mode == 0:
                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 255), 2) # rysowanie twarzy

            # Region twarzy do analizy oczu
            roi_gray = gray[y:y + h * 2 // 3, x:x + w] # Obszar twarzy (wycinek z obrazu szarość) do szukania oczu (górne 2/3)
            roi_color = frame[y:y + h * 2 // 3, x:x + w] # to samo w kolorze do rysowania
	\end{lstlisting}
\caption{Kod wykrywający twarz i wyznaczający ROI.}
\label{fig:kod-wykrywanie-twarzy}
\end{figure}

\subsection{Wykrywanie oczu i ich sortowanie}
\label{subsec:Wykrywanie-oczu-i-ich-sortowanie}

Metoda stosowana na rysunku \ref{fig:kod-wykrywanie-oczu} do wykrycia oczu jest identyczna jak omówiona w punkcie \ref{subsec:Wykrywanie-twarzy-i-wyznaczcie-ROI}. W tym przypadku obraz skalowany jest o mniejszą wartość, gwarantując dokładniejszą detekcję, co rekompensowane jest zwiększoną liczbą sąsiadów, odrzucającą wykryte tło. Wykryte oczy następnie sortowane są względem współrzędnej \texttt{x} -- oko z indeksem $0$ jest bliżej lewej strony rejonu zainteresowania. Następnie program przechodzi w pętli przez wykryte oczy, ograniczając się do pierwszych dwóch. Warto zauważyć, że w tym przypadku używana jest zmienna \texttt{i}, która pozwala na zapis danych binokularnych.

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		# Wykrywanie oczu
		eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.05, minNeighbors=10) # skalowanie obrazu 1.05, liczba trafień żeby uznać oczy 10 # eyes to prostokąty
		
		# Sortowanie oczu na podstawie pozycji x (lewe oko będzie miało mniejszą wartość x)
		eyes = sorted(eyes, key=lambda eye: eye[0]) # eyes to lista prostokątów oczu, key to funkcja która zwraca wartość do sortowania (tutaj x), lambda to funkcja anonimowa, eye to zmienna, eye[0] to współrzędna x

		
		# Dla każdego oka
		for i, (ex, ey, ew, eh) in enumerate(eyes[:2]):  # enumerate indeksuje sekwencję (i), a [:2] zwraca wycinek listy do 2 (bez niego, czyli 0 i 1) # współrzędne jak w twarzach
	\end{lstlisting}
\caption{Kod wykrywający oczy i sortujący je wzgledem pozycji.}
\label{fig:kod-wykrywanie-oczu}
\end{figure}

\subsection{Tryb kalibracji i punkt odniesienia}
\label{subsec:Tryb-kalibracji-i-punkt-odniesienia}

Fragment kodu, przedstawiony na rysunku \ref{fig:kod-punkt}, odpowiada za wyznaczenie środka oka na podstawie trzech różnych metod, zależnych od wyboru użytkownika. Pierwszy tryb kalibracji to kalibracja automatyczna, która jest domyślnie ustawiana po uruchomieniu programu. Punkt odniesienia wyznaczany jest w połowie szerokości i wysokości prostokątnego regionu \texttt{eye\-\_gray[i]}. Metoda ta może jednak tracić na dokładności przy obrocie głowy.

Drugi tryb to kalibracja ręczna, działająca na tej samej zasadzie co kalibracja automatyczna, lecz pozwalająca użytkownikowi na samodzielne określenie punktu odniesienia poprzez ustawienie go na aktualną pozycję źrenic.

Ostatnim trybem kalibracji jest tryb zaawansowany, który, wykorzystując progowanie obrazu i wykrywanie konturów (analogicznie do detekcji źrenic), pozwala na identyfikację odbicia rogówkowego. W założeniu obraz Purkinjego powinien być bardzo jasnym, lecz niewielkim punktem. Z tego powodu nie zastosowano operacji morfologicznych, które mogłyby go usunąć. Tryb ten jest trudny do wykorzystania przy użyciu światła białego, ponieważ musiałoby ono być na tyle intensywne, aby progowanie skutecznie usunęło oświetlony rejon twarzy bez usuwania odbicia od rogówki. Metoda ta jest znacznie bardziej odporna na ruch głowy, ale wymaga zastosowania dodatkowego sprzętu (np. kamery NIR i diody podczerwonej).

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		if calibration_mode == 0: # Obliczenie środka oka względem wycinka eye_gray

			eye_center_x[i] = ew // 2 # obliczenie współrzędnej x
			eye_center_y[i] = eh // 2 # obliczenie współrzędnej y
		elif calibration_mode == 1: # Obliczenie środka oka względem kalibracji ręcznej

			eye_center_x[i], eye_center_y[i] = calibrated_x[i], calibrated_y[i] # Skalibrowane współrzędne
		elif calibration_mode == 2: # Obliczenie środka oka względem odbicia światła
		
			# Progowanie odbicia światła w oku
			_, light_bin[i] = cv2.threshold(eye_gray[i], light_thresh, 255, cv2.THRESH_BINARY)

			# Znajdowanie konturów na obrazie binarnym odbicia światła
			light_countours, _ = cv2.findContours(light_bin[i], cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
								
			# Obliczenie środka oka względem odbicia światła (jeśli znaleziono kontury)
			if light_countours:
				# Znalezienie największego konturu odbicia światła
				largest_light_contour = max(light_countours, key=cv2.contourArea, default=None)

				# Wyznaczanie środka oka względem środka punktu odbicia światła
				(eye_center_x[i], eye_center_y[i]), _ = cv2.minEnclosingCircle(largest_light_contour)
			else:
				eye_center_x[i], eye_center_y[i] = None, None
	\end{lstlisting}
\caption{Kod prezentujący wybór trybu kalibracji.}
\label{fig:kod-punkt}
\end{figure}

\subsection{Binaryzacja, operacje morfologiczne i środek źrenic}
\label{subsec:Binaryzacja-operacje-morfologiczne-i-srodek-srenic}

Fragment kodu na rysunku \ref{fig:kod-binaryzacja} odpowiada za przetwarzanie obrazu binarnego oraz stosowanie operacji morfologicznych, opisanych w punkcie \ref{subsec:Binaryzacja-i-operacje-morfologiczne}. Zmienna używana do ustalenia progu binaryzacji jest wartością ustawioną na suwaku przez użytkownika. Na obrazie binarnym zapisywane są współrzędne konturów (kontury są wyznaczane bez aproksymacji, oznaczając każdy piksel).

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		# Progowanie obrazu (binaryzacja)
		_, eye_bin[i] = cv2.threshold(eye_gray[i], eye_thresh, 255, cv2.THRESH_BINARY_INV)

		# Tworzymy kernel (macierz do operacji morfologicznych)
		kernel = np.ones((3, 3), np.uint8)  # Możesz dostosować rozmiar

		# Usuwanie zakłóceń (otwarcie) - Usuwa szumy i małe zakłócenia, zachowuje główne struktury obiektów
		eye_bin_mopen = cv2.morphologyEx(eye_bin[i], cv2.MORPH_OPEN, kernel)

		# Wypełnianie małych dziur (zamknięcie) - Wypełnia dziury w obiektach, wygładza krawędzie obiektów (nie usuwa szumów)
		eye_bin_mopen_mclose[i] = cv2.morphologyEx(eye_bin_mopen, cv2.MORPH_CLOSE, kernel)

		# Znajdowanie konturów na obrazie binarnym
		eye_contours, _ = cv2.findContours(eye_bin_mopen_mclose[i], cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
	\end{lstlisting}
\caption{Kod przeprowadzający binaryzację i operacje morfologiczne na źrenicy.}
\label{fig:kod-binaryzacja}
\end{figure}

Jeśli udało się znaleźć kontury, to wyodrębniany jest największy z nich, a następnie wyznaczany jest jego środek przez funkcję znajdującą najmniejszy otaczający go okrąg. Punkt ten uznawany jest za środek źrenicy, względem którego obliczane jest przesunięcie względem punktu odniesienia. Przedstawiono to na rysunku \ref{fig:kod-zrenica}.

\begin{figure}[htbp]
	\centering
	\begin{lstlisting}
		if eye_contours:

		# Znaleziono kontury, wybieramy największy kontur
		largest_eye_contour = max(eye_contours, key=cv2.contourArea) # Największy kontur prawdopodobnie odpowiada źrenicy

		# Obliczenie środka i promienia okręgu otaczającego kontur
		(cx, cy), radius = cv2.minEnclosingCircle(largest_eye_contour) # Znajduje najmniejszy okrąg otaczający kontur i zwraca jego środek i promień (źrenicę)
	\end{lstlisting}
\caption{Kod wyznaczający środek wykrytej źrenicy.}
\label{fig:kod-zrenica}
\end{figure}







%Jeśli „Specyfikacja wewnętrzna”:
%\begin{itemize}
%\item przedstawienie idei -> wykonanie niezbędnych obliczeń projektowych, syntezy układu
%\item architektura systemu -> opis architektury proponowanego rozwiązania
%\item opis struktur danych (i organizacji baz danych)
%\item komponenty, moduły, biblioteki, przegląd ważniejszych klas (jeśli występują) -> oprogramowania systemu
%\item przegląd ważniejszych algorytmów (jeśli występują) !!!
%\item szczegóły implementacji wybranych fragmentów, zastosowane wzorce projektowe !!!
%\item diagramy UML
%\end{itemize}



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Pakiet minted wymaga importu: \usepackage{minted}                 %
% i specjalnego kompilowania:                                       %
% pdflatex -shell-escape main                                       %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


%Krótka wstawka kodu w linii tekstu jest możliwa, np.  \lstinline|int a;| (biblioteka \texttt{listings})% lub  \mintinline{C++}|int a;| (biblioteka \texttt{minted})
. 
%Dłuższe fragmenty lepiej jest umieszczać jako rysunek, np. kod na rys \ref{fig:pseudokod:listings}% i rys. \ref{fig:pseudokod:minted}
%, a naprawdę długie fragmenty – w załączniku.


%\begin{figure}
%\centering
%\begin{lstlisting}
%class test : public basic
%{
%    public:
%      test (int a);
%      friend std::ostream operator<<(std::ostream & s, 
%                                     const test & t);
%    protected:
%      int _a;  
%      
%};
%\end{lstlisting}
%\caption{Pseudokod w \texttt{listings}.}
%\label{fig:pseudokod:listings}
%\end{figure}

%\begin{figure}
%\centering
%\begin{minted}[linenos,frame=lines]{c++}
%class test : public basic
%{
%    public:
%      test (int a);
%      friend std::ostream operator<<(std::ostream & s, 
%                                     const test & t);
%    protected:
%      int _a;  
%      
%};
%\end{minted}
%\caption{Pseudokod w \texttt{minted}.}
%\label{fig:pseudokod:minted}
%\end{figure}




%--------------------------------------
%
%
% TODO: 
%
% Status: 99% (basdab DONE)
%--------------------------------------
\chapter{Weryfikacja i walidacja}
\label{ch:Weryfikacja-i-walidacja}

\section{Sposób testowanie w ramach pracy}
\label{sec:Sposob-testowanie-w-ramach-pracy}

Projekt był realizowany w sposób zwinny, podejście Scrum wspomniane punkcie \ref{subsec:Metodyka-pracy-nad-projektem}, jest w stanie po części opisać charakter pracy nad nim, jako że co pewien czas przeprowadzano większą analizę i planowanie dalszych kroków. Jednakże wynikało to w większości z potrzeby poszerzenia wiedzy o możliwych rozwiązaniach, by umożliwić następne iteracje. Metodyką, która najlepiej opisuje samo programowanie i testowanie rozwiązania, jest programowanie ekstremalne (XP ang. \english{eXtreme Programming}) w wersji z mniej ścisłymi praktykami. Programowanie to skupia się na częstym i jak najwcześniejszym wydawaniu, otwartości na zmienianie (również gruntowne) kodu w miarę zmieniających się wymagań oraz dogłębnym testowaniu jednostkowym. Takie podejście pozwala na efektywne tworzenie rozwiązania bez wcześniejszej wiedzy, jak go prawidłowo wykonać, poprzez obserwację innych projektów, które odniosły sukces, i łączenie wielu metod w jedną całość.

Testowanie przeprowadzano jak najczęściej, od razu po wprowadzeniu nowej funkcjonalności, sprawdzając jej standardowe działanie, spójność z resztą systemu oraz pracę w skrajnych warunkach. Testy jednostkowe, integracyjne i akceptacyjne wykonywane były manualnie. W przypadku wykrycia błędu, problemu ze spójnością lub niespełnienia oczekiwanych standardów dalsza rozbudowa systemu była wstrzymywana do momentu usunięcia problemu. Po każdej poprawce przeprowadzano testy regresyjne, aby upewnić się, że wcześniej działające funkcjonalności nie zostały przypadkowo zepsute.

\section{Przypadki testowe}
\label{sec:Przypadki-testowe}

W przypadku wszystkich testów używano tego samego sprzętu, opisanego w podrozdziale \ref{sec:Narzedzia-i-metody-wykorzystane-w-pracy}, za wyjątkiem testu działania plików wykonywalnych na innym laptopie. 

\subsection{Testowanie uniwersalności dla innych twarzy (niepełne)}
\label{subsec:Testowanie-twarzy}

Działanie systemu wizyjnego do śledzenia ruchu gałek ocznych testowano w większości przypadków na autorze pracy. Jednakże zarówno w trakcie rozwoju, jak i po ukończeniu rozwiązania prezentowano jego działanie innym osobom. Pozwoliło to na określenie konieczności ograniczenia rejonu twarzy w celu wykrycia oczu. Potwierdzono również uniwersalność detekcji twarzy, oczu i źrenic względem różnych użytkowników. Rezultat ten był spodziewany, jako że klasyfikator Haara był dostarczonym, wyuczonym i przetestowanym narzędziem przez OpenCV, a źrenice w obrazie monochromatycznym są z natury najciemniejszym punktem na twarzy. Wymagane jest przeprowadzenie detekcji na grupie badawczej większych rozmiarów, by definitywnie potwierdzić ten aspekt rozwiązania.

\subsection{Testowanie odporności na różne oświetlenie (pełne)}
\label{subsec:Testowanie-oswietlenie}

By nie uzależnić śledzenia od konkretnych warunków oświetleniowych, badano zachowanie systemu w różnych środowiskach na całym etapie jego rozwoju. Do testów użyto światła naturalnego, sztucznego o różnej barwie oraz ich kombinacji. Źródła światła były umieszczane w różnych miejscach względem użytkownika, badając wpływ kierunku padania światła. Przeprowadzono testy ze światłem padającym od góry, od frontu, od tyłu oraz od lewego i prawego boku. Sprawdzono również limitacje związane z natężeniem oświetlenia, zarówno przy silnym naświetleniu, jak i w półmroku. Prezentowane rozwiązanie działa najmniej stabilnie przy oświetleniu padającym od boków i centralnie od góry z powodu cienia na jednej połowie twarzy kontrastującego z silnym naświetleniem drugiej połowy. Mimo to ukończony system pozwala na śledzenie niezależnie od oświetlenia, z wyjątkiem skrajnych warunków.

\subsection{Testowanie ustawień klasyfikatora Haara (niepełne)}
\label{subsec:Testowanie-funkcji}

Istotną częścią działania systemu jest ograniczanie obszaru detekcji źrenic do twarzy i oczu na ekranie. Obie detekcje korzystają z tej samej funkcji biblioteki OpenCV (\texttt{cv2.\-Cascade\-Classifier.\-detectMultiScale()}), która wymaga ustalenia współczynnika skalowania i minimalnej liczby sąsiadów pozytywnej detekcji. Współczynnik skalowania wpływa na szanse detekcji i szybkość wykonywania, a liczba sąsiadów – na ilość detekcji i ich jakość. Wartości tych parametrów zostały określone przez wyszukiwanie wyczerpujące, modyfikując je tak długo, aż obserwowany rezultat spełniał stawiane wymagania. Tak dobrane parametry pozwoliły na sprawną detekcję, co może być zasługą solidności klasyfikatorów Haara dostępnych w bibliotece OpenCV, ale nie gwarantują optymalności. 

\subsection{Testowanie poprawności wyliczonych wartości}
\label{subsec:Testowanie-wartosci}

Kod programu posiada zabezpieczenie przed obliczaniem pozycji źrenicy względem punktu odniesienia, jeśli chociaż jeden z tych elementów nie został wyznaczony. Przeprowadzono testy, celowo przysłaniając kamerę oraz część twarzy, aby uzyskać efekt braku detekcji lub detekcji częściowej. Potwierdziły one skuteczność tych warunków. Jedynym wyjątkiem jest sytuacja wykrycia jedynie jednego oka znajdującego się po prawej stronie obrazu. W tym przypadku interpretowane jest ono jako oko po stronie lewej. Jest to spowodowane indeksowaniem oczu w kolejności występowania, licząc od lewej strony obszaru twarzy. Warto zaznaczyć, że osiągnięcie takiej sytuacji było trudne, jako że w momencie przysłonięcia zbyt dużej ilości twarzy detekcja zostaje przerwana. Ten aspekt pomiaru został przetestowany w sposób pełny.

W sposób niepełny sprawdzono dokładność obliczenia pozycji źrenicy i punktu odniesienia. Wyznaczenie środka źrenicy zostało sprawdzone empirycznie, obserwując rysowany punkt i obrys na obrazie. Stwierdzono, że jakość pomiaru zależy od ustawienia progu binaryzacji -- zwiększanie tej wartości pozwala na pewniejszą detekcję, ale grozi interpretacją cienia lub tęczówki jako źrenicy, co znacząco wpłynie na jakość obliczeń. Punkt odniesienia również był oznaczany na obrazie z możliwością ustawienia jego położenia kalibracją ręczną. Oprócz empirycznego ustalenia poprawności wyznaczania punktu odniesienia przeprowadzono badanie, w którym skupiano wzrok na jednym punkcie przy unieruchomionej głowie. Następnie użyto kalibracji ręcznej i po krótkim czasie zakończono działanie programu. Wynik badania widoczny jest na rysunku \ref{fig:test-pomiary}. Można zaobserwować, że na końcu sesji śledzenia źrenica oscylowała blisko środka punktu $(0,0)$ -- punktu odniesienia -- co wskazuje na poprawność wykonywanych obliczeń, jako że źrenica w momencie skupienia uwagi jest praktycznie nieruchoma.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{pic/testy/test punktu odniesienia.png}
	\caption{Wykres czasowy sesji testującej dokładność wykonywanych pomiarów.}
	\label{fig:test-pomiary}
\end{figure}

\subsection{Testowanie zapisywania do pliku (pełne)}
\label{subsec:Testowanie-zapisywania}

Zapisywanie do pliku warunkowane jest w ten sam sposób, jak wyznaczenie pozycji źrenicy względem punktu odniesienia w punkcie \ref{subsec:Testowanie-wartosci}. Sprawdzenie poprawności działania zapisu do pliku zostało w związku z tym przeprowadzone równolegle z badaniem wyliczonych wartości.

\subsection{Testowanie integralności i stabilności (pełne)}
\label{subsec:Testowanie-interakcji}

Testy integralności były przeprowadzane przez cały proces tworzenia rozwiązania, sprawdzając wszystkie możliwe kombinacje, przechodząc od jednej funkcjonalności do każdej innej. Wszystkie funkcje systemu śledzącego są stabilne i mogą być dowolnie zmieniane w trakcie sesji. Jedynym niezamierzonym błędem jest możliwość zamrożenia działania programu w momencie przenoszenia któregokolwiek z jego okien myszą. Podobny efekt powstaje w momencie przytrzymania dowolnego klawisza na klawiaturze -- obraz w oknie odświeża się znacznie wolniej (jest zamrożony), z tą różnicą że program wciąż działa w swojej typowej częstotliwości ($\numrange{20}{30} \ \unit{\hertz}$), co zostało ustalone na podstawie analizy różnic momentów czasowych pomiędzy wykonanymi pomiarami.

\subsection{Testowanie kalibracji zaawansowanej (niepełne)}
\label{subsec:Testowanie-zaawansowanej}

Funkcjonalność kalibracji zaawansowanej nie została w pełni potwierdzona z powodu limitacji sprzętowych. Przeprowadzono test, w którym użyto intensywnego źródła światła białego, ustawionego od frontu, a następnie próbowano dobrać poziom progowania tak, żeby na obrazie binarnym pozostało jedynie odbicie rogówkowe. Próba ta zakończyła się niepowodzeniem, ponieważ wraz ze zwiększaniem intensywności naświetlenia białka oczne były coraz trudniejsze do usunięcia. Badanie to było także bardzo irytujące dla oczu, co wskazuje na mały potencjał zastosowania tej kalibracji w widzialnym spektrum. Obraz binarny w trakcie testu widoczny jest na rysunku \ref{fig:test-zaawansowana}. Istnieje możliwość, że funkcjonalność ta nie spełni swojej roli nawet operując w bliskiej podczerwieni, ponieważ poszukuje największego konturu na obrazie binarnym, a bez jego przetworzenia całkowite usunięcie innych wykrytych obiektów może być niemożliwe.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{pic/testy/zaawansowana.png}
	\caption{Obraz binarny odbicia rogówkowego, w trakcie próby testującej kalibrację zaawansowaną w widzialnym spektrum światła.}
	\label{fig:test-zaawansowana}
\end{figure}

\FloatBarrier
\section{Wykryte i usunięte błędy}
\label{sec:wykryte-i-usunięte-bledy}

\begin{description}
	\item[Detekcja otworów nosowych i ust jako oczu --] Pierwszym napotkanym błędem było wykrywanie fałszywych pozytywów w rejonach otworów nosowych i ust. Rozwiązano go, ograniczając rejon detekcji oczu do $\frac{2}{3}$ twarzy, licząc od góry, a także dobierając odpowiednie wartości parametrów \texttt{cv2.\-Cascade\-Classifier.\-detectMultiScale()}.
	\item[Niestabilna detekcja źrenic --] W początkowych fazach program cierpiał na problem z detekcją źrenic w momencie obrotu oczu zbyt daleko w jednym kierunku. Jest to ogólny problem okulografów opartych na analizie wideo, ponieważ źrenica może stać się całkowicie niewidoczna na obrazie kamery. Próba rozwiązania tego błędu z użyciem przepływu optycznego wprowadziła większą niestabilność, a przede wszystkim dezorientację i utrudniła szukanie źródła błędu w poprawnie działających funkcjonalnościach. Dopiero w późniejszych etapach rozwoju osiągnięto wykrywanie źrenic spełniające stawiane wymagania. Problem rozwiązany został przez odpowiednie operacje morfologiczne, wizualizację obrazu binarnego i możliwość zmiany wartości progu w trakcie pracy programu.
	\item[Problem detekcji z przepływem optycznym --] Użycie optycznego przepływu okazało się rozwiązaniem mało praktycznym, a przede wszystkim trudnym w implementacji w sposób wspomagający detekcję. Możliwe, że odpowiednie wykorzystanie tego narzędzia mogłoby rozszerzyć możliwości programu, ale z powodu prostej i niezawodnej alternatywy porzucono jego zastosowanie na rzecz binaryzacji i operacji morfologicznych.
	\item[Wyznaczanie konturów --] Funkcja \texttt{cv2\-.find\-Contours()} z opcją CHAIN\-\_APPROX\-\_SIMPLE zwracała uproszczone kontury, reprezentując proste linie pikseli jedynie za pomocą punktu początkowego i końcowego. Powodowało to niestabilność w działaniu algorytmu, którą skorygowano poprzez zastosowanie opcji CHAIN\-\_APPROX\-\_NONE, zwracającej wszystkie punkty konturu. Brak aproksymacji kształtu źrenicy nie miał istotnego wpływu na działanie programu.
	\item[Błędne obliczenia pozycji źrenicy --]  Funkcja \texttt{cv2\-.minEnclosing\-Circle()} zwraca środek i promień najmniejszego okręgu, który jest w stanie objąć podany kontur. Kontur jest zbiorem współrzędnych pikseli danego obiektu na danym obrazie. W związku z tym wyznaczony środek podaje współrzędne względem tego obrazu. W tym przypadku jest to wycinek klatki z kamery przedstawiający oko. Błąd w wyliczaniu pozycji źrenicy wynikał z pomieszania współrzędnych dotyczących innych fragmentów obrazu z kamery.
	\item[Powielenie nazwy suwaków --] Suwaki dostępne z biblioteki OpenCV muszą mieć odrębne nazwy.
	\item[Kalibracja ręczna działa monokularnie --] Podczas próby kalibracji ręcznej punkt odniesienia ustawiany był względem jednej, losowo wykrytej źrenicy. Było to spowodowane wykrywaniem oczu pojedynczo i bez ich rozróżniania. Problem rozwiązany został w momencie implementacji zmiennych binokularnych oraz sortowania wykrytych oczu względem ich pozycji.
	\item[Błąd kompilacji pliku wykonywalnego na innej maszynie --] Początkowe użycie PyInstallera nie dołączało klasyfikatorów Haara do pliku wykonywalnego, co powodowało błąd uniemożliwiający uruchomienie programu. Dopiero zastosowanie komendy z opcją \texttt{-{}-add-data}, po której wprowadzono ścieżkę do plików XML, pozwoliło na łatwiejsze przenoszenie programu śledzącego.
\end{description}








%--------------------------------------
%
%
% TODO: 
%
% Status: 100% (DONE!!!!!!)
%--------------------------------------
\chapter{Podsumowanie i wnioski}

\section{Podsumowanie wyników w świetle postawionych wymagań}

System skutecznie analizuje obraz w czasie rzeczywistym, co potwierdzają testy wydajności procesora i pamięci RAM. Nie zaobserwowano istotnych opóźnień w przetwarzaniu, co wskazuje na spełnienie tego wymagania. Proces śledzenia ruchu gałek ocznych przebiega zgodnie z założeniami kaskadowej detekcji, wykrywając wpierw twarz, następnie oczy i na końcu źrenice. System działa w sposób binokularny, co oznacza, że śledzi ruch obu oczu jednocześnie, jednakże w rzadkich przypadkach może dojść do błędnej interpretacji jednego z nich. Ryzyko wystąpienia takiej sytuacji jest niewielkie, ale nie niemożliwe.

System zapewnia uniwersalność wykrywania źrenic w różnych warunkach oświetleniowych, dzięki możliwości ręcznej regulacji progu binaryzacji. Ponadto, umowny punkt odniesienia został zaimplementowany w sposób odporny na niewielkie ruchy głowy. Możliwe jest także skalibrowanie owego punktu w prosty sposób. Wprowadzona została również bardziej zaawansowana metoda kalibracji, jednak jej skuteczność nie została jeszcze w pełni zweryfikowana, co oznacza, że spełnia to wymaganie jedynie częściowo. Funkcja zapisu pomiarów do pliku została przetestowana i działa poprawnie, zapewniając integralność i poprawność rejestrowanych danych.

Pod względem wymagań niefunkcjonalnych system również spełnia kluczowe założenia. Obraz z kamery jest wyświetlany w oknie programu w czasie rzeczywistym, co umożliwia użytkownikowi bieżącą obserwację procesu śledzenia. Dodatkowo, program został wyposażony w mechanizm kontroli błędów, który informuje użytkownika o ewentualnych problemach, co znacząco ułatwia diagnostykę. Możliwość łatwego przełączania poszczególnych funkcji pozwala na elastyczne dostosowanie działania systemu do potrzeb użytkownika.

W zakresie czytelności wizualizacji program oferuje obrysowywanie obszarów detekcji z opcją wyboru trybu rysowania, co zwiększa komfort użytkowania. Dodatkowo, użytkownik ma możliwość podglądu technicznych aspektów wykrywania, w tym binarnych obrazów źrenic i odbicia rogówkowego dla obu oczu, co pozwala na analizę działania systemu w czasie rzeczywistym. Istnieje również zewnętrzne narzędzie, \texttt{eye\-\_tracking\-\_plot.exe}, umożliwiające wizualizację wyników w postaci wykresów czasowych, co stanowi dodatkowe wsparcie w analizie zebranych danych.

\section{Potencjalne kierunki rozwoju}

Rozwój systemu w jego obecnej formie może napotkać pewne ograniczenia, wynikające z omówionych w podrozdziale \ref{sec:Architektura-systemu} aspektów architektonicznych. Dlatego kluczowym krokiem w dalszym udoskonalaniu rozwiązania powinna być zmiana jego struktury na bardziej modularną, co ułatwi modyfikację i rozszerzanie funkcjonalności.

Ważnym kierunkiem rozwoju jest również przetestowanie działania systemu w bliskiej podczerwieni, co mogłoby poprawić skuteczność wykrywania źrenic w trudniejszych warunkach oświetleniowych. Dodatkowo, konieczne jest przeprowadzenie dokładniejszych testów zaawansowanej kalibracji, aby potwierdzić jej poprawność i niezawodność, zaczynając od właśnie pracy w podczerwieni.

Interfejs użytkownika również ma duży potencjał do usprawnienia. Jego dalszy rozwój może zwiększyć intuicyjność obsługi oraz poprawić ogólne doświadczenie użytkownika.

Najbardziej wartościowym kierunkiem rozwoju jest jednak implementacja mechanizmu bezpośredniego wyznaczania punktu spojrzenia. Obecnie system dostarcza dane umożliwiające jego określenie, ale nie wykonuje tej operacji automatycznie. Wprowadzenie takiej funkcji mogłoby znacząco zwiększyć użyteczność systemu, czyniąc go bardziej kompletnym narzędziem do analizy ruchu gałek ocznych. Taka funkcjonalność pozwoliła by również na bardziej bezpośrednie wyznaczenie dokładności wykonywanych pomiarów.

\section{Problemy napotkane w trakcie pracy}

Największym wyzwaniem podczas realizacji projektu okazało się zarządzanie czasem. Praca nad tak złożonym systemem w pojedynkę, przy ograniczonym harmonogramie, wymagała dużej samodyscypliny oraz umiejętności bieżącej oceny postępów i priorytetów.

Jednym z trudniejszych aspektów było znalezienie równowagi między dążeniem do doskonałości a koniecznością ukończenia kluczowych funkcjonalności w wyznaczonym czasie. Gdy dostrzega się potencjał do ulepszeń, trudno zrezygnować z dalszej optymalizacji, ale jednocześnie nadmierne skupienie na dopracowywaniu szczegółów może prowadzić do zaniedbania istotnych elementów systemu.

Ostatecznie, konieczne było świadome podejmowanie decyzji o zakresie realizowanych funkcji, tak aby system był jak najbardziej kompletny i użyteczny, nawet jeśli nie wszystkie jego aspekty zostały rozwinięte w maksymalnym stopniu.




\backmatter

%\bibliographystyle{plplain}  % bibtex
%\bibliography{biblio} % bibtex
\printbibliography           % biblatex
\addcontentsline{toc}{chapter}{Bibliografia}

\begin{appendices}

%--------------------------------------
%
%
% TODO: 
%
% Status: 100%
%--------------------------------------
\chapter{Spis skrótów i symboli}

\begin{itemize}
\item[EOG] elektro-okulografia (ang. \english{electrooculography})
\item[POV] punkt widzenia (ang. \english{Point of View})
\item[IR] podczerwień (ang. \english{Infrared})
\item[DPI] podwójny obraz Purkinjego (ang. \english{Dual Purkinje Image})
\item[UML] zunifikowany język modelowania (ang. \english{Unified Modeling Language})
\item[VS Code] Visual Studio Code
\item[CSV] wartości rozdzielone przecinkiem (ang. \english{comma-separated values}) 
\item[EXE] wykonywalny (ang. \english{executable})
\item[RAM] pamięć o dostępie swobodnym (ang. \english{random-access memory})
\item[USB] uniwersalna magistrala szeregowa (ang. \english{Universal Serial Bus})
\item[NIR] Bliska podczerwień (ang. \english{Near-InfraRed})
\item[ASCII] Amerykański standardowy kod dla wymiany informacji (ang. \english{American Standard Code for Information Interchange})
\item[TXT] plik tekstowy (ang. \english{text file}) 
\item[XML] rozszerzalny język znaczników (ang. \english{Extensible Markup Language})
\item[XP] programowanie ekstremalne (ang. \english{eXtreme Programming})
\item[$x_t$] wyjście systemu dyskretnej chwili czasu
\item[$s_t$] wejście systemu w dyskretnej chwili czasu
\item[$e_t$] uchyb systemu w dyskretnej chwili czasu
\item[$g$] współczynnik filtra liniowego różniczkującego
\item[$h$] współczynnik filtra liniowego
\item[$P$] Ocena PassMark 
\item[$CPU_{\%}$] Procent wykorzystania zasobów procesora
\item[$\mathcal{Z}$] transformata Z
\item[$\frac{X(z)}{S(z)}$] transmitancją układu dyskretnego
\item[$i(x,y)$] wartość piksela w oryginalnym obrazie na współrzędnych $(x,y)$
\item[$I(x,y)$] wartość piksela w obrazie całkowym na współrzędnych $(x,y)$
\item[$\texttt{dst}(x,y)$] wartość piksela w obrazie wynikowym po binaryzacji
\item[$\texttt{src}(x,y)$] wartość piksela w obrazie wejściowym przed binaryzacją
\item[$\texttt{thresh}$] wartość progowa (próg binaryzacji)
\item[$\texttt{maxval}$] maksymalna wartość jasności

\end{itemize}


%--------------------------------------
%
%
% TODO: długi fragment źródła
%
% Status: 100%
%--------------------------------------
\chapter{Źródła}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Pakiet minted wymaga odkomentowania w pliku config/settings.tex   %
% importu pakietu minted: \usepackage{minted}                       %
% i specjalnego kompilowania:                                       %
% pdflatex -shell-escape praca                                      %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

%\begin{minted}[linenos,breaklines,frame=lines]{c++}
%if (_nClusters < 1)
%   throw std::string ("unknown number of clusters");
%if (_nIterations < 1 and _epsilon < 0)
%   throw std::string ("You should set a maximal number of iteration or minimal difference -- epsilon.");
%if (_nIterations > 0 and _epsilon > 0)
%   throw std::string ("Both number of iterations and minimal epsilon set -- you should set either number of iterations or minimal epsilon.");
%\end{minted}


%--------------------------------------
%
%
% TODO: 
%
% Status: 100%
%--------------------------------------
\chapter{Lista dodatkowych plików, uzupełniających tekst pracy} 


W systemie do pracy dołączono dodatkowe pliki zawierające:
\begin{itemize}
\item źródła programu,
\item pliki wykonywalne,
\end{itemize}


\listoffigures
\addcontentsline{toc}{chapter}{Spis rysunków}
\listoftables
\addcontentsline{toc}{chapter}{Spis tabel}

\end{appendices}

\end{document}


%% Finis coronat opus.

